\documentclass[pdftex,12pt,a4paper]{article}

\input{/home/boris/science/tex_general/title_bor_utf8}

% чисто эконометрические сокращения:
\def \hb{\hat{\beta}}
\def \hy{\hat{y}}
\def \hY{\hat{Y}}
\def \he{\hat{\varepsilon}}
\def \hCorr{\widehat{\Corr}}
\def \hVar{\widehat{\Var}}
\def \hCov{\widehat{\Cov}}


\begin{document}
\parindent=0 pt % отступ равен 0

\listoftodos

\section{Конвенция об обозначениях}

\begin{enumerate}
\item $y$ --- вектор-столбец зависимых переменных размера $(n \times 1)$, наблюдаемый случайный

\item $\beta$ --- вектор-столбец неизвестных коэффициентов размера $(k \times 1)$, ненаблюдаемый, неслучайный

\item $\hy$ --- вектор столбец прогнозов для $y$, полученных по некоторой модели, размера $(n \times 1)$, наблюдаемый, случайный

\item $\hb$ --- вектор-столбец оценок $\beta$ размера $(k \times 1)$, наблюдаемый, случайный

\item $X$ --- матрица всех объясняющих переменных, размера $(n \times k)$. Известная, стохастическая или детерминированная в зависимости от парадигмы.

\item $\e$ --- вектор-столбец случайных ошибок размера $(n \times 1)$, ненаблюдаемый случайный

\item $\he$ --- вектор-столбец остатков модели размера $(n \times 1)$, наблюдаемый случайный
\end{enumerate}

В некоторых учебниках используется обозначение $Y$ для исходного вектора зависимых переменных, а $y$ --- для центрированного, т.е.  $y=Y-\bar{Y}$. В этом документе $y$ обозначает исходный вектор $y$. 


\section{Семинар 1}

Неформальное определение. Если матрица $A$ квадратная, то её определителем называется площадь/объём параллелограмма/параллелепипеда образованного векторами-столбцами матрицы. Знак определителя задаётся порядком следования векторов. 


Свойства определителя:
\begin{enumerate}
\item $\det(AB)=\det(A)\det(B)=\det(BA)$, если $A$ и $B$ квадратные
\item $\det(A)=\prod \lambda_i$, где $\lambda_i$ --- собственное число матрицы $A$, возможно комплексное.
\end{enumerate}


Определение. Ненулевой вектор $x$ называется собственным вектором матрицы $A$, если при умножении на матрицу $A$ он остается на той же прямой, т.е. $Ax=\lambda x$.

Определение. Число $\lambda$ называется собственным числом матрицы $A$, если существует вектор $x$, который при умножении на матрицу $A$ изменяется в $\lambda$ раз, т.е. $Ax=\lambda x$.

Определение. Если матрица $A$ квадратная, то её следом называется сумма диагональных элементов, $\trace(A)=\sum a_{ii}$.

Свойства следа:
\begin{enumerate}
\item $\trace(A+B)=\trace(A)+\trace(B)$
\item $\trace(AB)=\trace(BA)$, если $AB$ и $BA$ существуют. При этом $A$ и $B$ могут не быть квадратными матрицами.
\item $\trace(A)=\sum \lambda_i$, где $\lambda_i$ --- собственное число матрицы $A$, возможно комплексное.
\end{enumerate}


Смысл следа. Если умножение на матрицу $A$ --- это проецирование, то есть $Ax$ --- есть проекция вектора $x$ на некоторое подпространство, то $\trace(A)$ --- размерность этого подпространства. Действительно, если $A$ --- проектор, то $A^2=A$ и собственные числа матрицы $A$ равны нулю или единице. Поэтому $\trace(A)$ равен количеству собственных чисел равных единице. И, следовательно, $\trace(A)$ равен $\rank(A)$, то есть размерности пространства, на которое матрица $A$ проецирует вектора. У следа матрицы есть и другие смыслы. 


Добавить про геометрический смысл следа, \url{http://mathoverflow.net/questions/13526/geometric-interpretation-of-trace}.







Метод наименьших квадратов (МНК), ordinary least squares (OLS):


Есть $n$ наблюдений, $y_1$, ..., $y_n$. Есть модель, которая даёт прогнозы, $\hat{y}_1$, ..., $\hat{y}_n$. Эта модель зависит от вектора неизвестных параметров, $\beta$. МНК предлагает в качестве оценок неизвестных параметров взять такое $\hb$, чтобы минимизировать $\sum (y_i-\hat{y}_i)^2$.

\section{Семинар 2}

Контрольная-1

\section{Картинка}


Утверждение. $\sCorr^2(y,\hy)=R^2$

Доказательство. По определению, $\sCorr(y,\hy)=\frac{(y-\bar{y})(\hy-\bar{\hy})}{|y-\bar{y}||\hy-\bar{\hy}|}$. Поскольку в регрессии присутствует свободный член, $\bar{\hy}=\bar{y}$. Значит, 
\begin{equation}
\sCorr(y,\hy)=\frac{(y-\bar{y})(\hy-\bar{y})}{|y-\bar{y}||\hy-\bar{y}|}=\cos(y-\bar{y},\hy-\bar{y})
\end{equation}
По определению, $R^2=\frac{|\hy-\bar{y}|^2}{|y-\bar{y}|^2}=\cos^2(y-\bar{y},\hy-\bar{y})$



Опыт: лучший результат у меня получается с обозначением $(\bar{y},\ldots,\bar{y})$.


\section{Мегаматрица}
 
\begin{theorem}
След и математическое ожидание можно переставлять, $\E(\tr(A))=\tr(\E(A))$.
\end{theorem} 

\begin{theorem}
Математическое ожидание квадратичной формы
\begin{equation}
\E(x'Ax)=\tr(A\Var(x))+\E(x')A\E(x)
\end{equation}
\end{theorem}
\begin{proof}
Мы будем пользоваться простым приёмом. Если $u$ --- это скаляр, вектор размера 1 на 1, то $\tr(u)=u$.

Поехали,
\begin{equation}
\E(x'Ax)=\E(\tr(x'Ax))=\E(\tr(Axx'))=\tr(\E(Axx'))=\tr(A\E(xx'))
\end{equation}

По определению дисперсии, $\Var(x)=\E(xx')-\E(x)\E(x')$. Поэтому:
\begin{equation}
\tr(A\E(xx'))=\tr(A(\Var(x)+\E(x)\E(x')))=\tr(A\Var(x))+\tr(A\E(x)\E(x'))
\end{equation}

И готовимся снова использовать приём $\tr(u)=u$:
\begin{equation}
\tr(A\Var(x))+\tr(A\E(x)\E(x'))=\tr(A\Var(x))+\tr(\E(x')A\E(x))=
\tr(A\Var(x))+\E(x')A\E(x)
\end{equation}

\end{proof}


\section{Парадигма Случайных величин}

В парадигме случайных величин накладывают разные предпосылки.


Пример 1. Ошибки измерения в регрессорах


Пример 2. Независимые наблюдения


Пример 3. Стационарный процесс




Обозначим $X_{i.}$ --- $i$-ая строка матрицы $X$.

Вариант 0.

\begin{enumerate}
\item Регрессоры $X_{i.}$, относящиеся к разным $i$ некоррелированы.
\item Ковариационная матрица $X_{i.}$ не зависит от $i$.
\item Зависимая переменная представима в виде $y_i=X_{i.}\beta+\e_i$
\item Величины $\e_i$ некоррелированы, $\E(\e_i)=0$, $\Var(\e_i)=\sigma^2$.
\item $\Cov(\e_i,x_{ij})=0$ для всех $i$ и $j$
\item Вероятность полного ранга матрицы $X$ равна единице
\end{enumerate}

При выполнении этих предпосылок оценки МНК существуют с вероятностью 1 

Оценки состоятельны
\begin{proof}
Разложим $\hb$ в виде $\hb=(X'X)^{-1}X'y=(X'X)^{-1}X'(X\beta+\e)=\beta+(X'X)^{-1}X'\e$

Заметим, что $(X'X)^{-1}X'\e=\left(\frac{1}{n}X'X\right)^{-1}\frac{1}{n}X'\e$.

$\plim \left(\frac{1}{n}X'X\right)=Var(X_{i.})$

$\plim \frac{1}{n}X'\e=0$
\end{proof}




Сравнение двух парадигм

\begin{tabular}{c|cc}
 & детерминированные $X$  & случайные $X$ \\ 
\hline 
$\E(y_i)$ & разные, $X_{i.}\beta$ & одинаковые \\ 
$sVar(y)$ --- несмещенная оценка для $Var(y_i)$ & Нет & Да \\
 
\end{tabular} 

\section{Разное}
\begin{enumerate}

\item Гипотеза $H_0$ по-английски читается как <<H naught>>
\item При проверке гипотезы об адекватности регрессии НЕЛЬЗЯ писать $H_0: R^2=0$. 


Гипотезы имеет смысл проверять о ненаблюдаемых неизвестных константах. 
Проверить гипотезу о том, что $R^2=0$ легко. 
Для этого не нужно знать ничего из теории вероятностей, достаточно просто сравнить посчитанное значение $R^2$ с нулём. 

Более того, даже корректировка $\E(R^2)=0$ неверна. 
Случайная величина $R^2$ всегда неотрицательна, поэтому при любых разумных предпосылках на $\varepsilon$ окажется, что $\P(R^2>0)>0$. 
А это приведёт к тому, что $\E(R^2)>0$ даже если $Y$ никак не зависит от $X$.


Единственный правильный вариант --- $H_0: \beta_2=\beta_3=\ldots=\beta_k=0$ и $H_a: \exists i\geq 2 : \beta_i\neq 0$.

Можно добавить, что при построении регрессии $\hy=\hb_1$ величина $R^2$ тождественно равна нулю, вне зависимости от того, чему на самом деле равен $y$. Но эту гипотезу тоже не надо проверять, ведь мы это точно знаем.
\end{enumerate}



\section{Ridge/Lasso regression}

LASSO --- Least Absolute Shrinkage and Selection Operator. Метод построения регрессии, предложенный Robert Tibshirani в 1995 году.

Вспомним обычный МНК:
\begin{equation}
\min_{\beta} (y-X\beta)'(y-X\beta)
\end{equation}


LASSO вместо исходной задачи решает задачу условного экстремума:
\begin{equation}
\min_{\beta} (y-X\beta)'(y-X\beta)
\end{equation}
при ограничении $\sum_{j=1}^{k}|\beta_j|\leq c$.

\todo[inline]{Проверить! Нет ли у $\beta_1$ особого положения?}

Естественно, при больших значениях $c$ результат LASSO совпадает с МНК. Что происходит при малых $c$?


Для наглядности рассмотрим задачу с двумя коэффициентами $\beta$: $\beta_1$ и $\beta_2$. Линии уровня целевой функции --- эллипсы. Допустимое множество имеет форму ромба с центром в начала координат.


\todo[inline]{на картинке три $c$: очень большое --- дающиее мнк решение, меньше --- ненулевые $\beta$, маленькое --- одна из $\beta$  равна 0}


То есть при малых $c$ LASSO обратит ровно в ноль некоторые коэффициенты $\beta$.


Применим метод множителей Лагранжа для случая, когда ограничение $\sum_{j=1}^{k}|\beta_j|\leq c$ активно, то есть выполнено как равенство. 

\begin{equation}
L(\beta,\lambda)=(y-X\beta)'(y-X\beta)+\lambda \left(\sum_{j=1}^{k}|\beta_j| - c \right)
\end{equation}

Необходимым условием первого порядка является $\partial L/\partial \beta =0$. 
Это условие первого порядка не изменится, если мы зачеркнём $c$ в выражении. 
Таким образом мы получили альтернативную формулировку метода LASSO:
\begin{equation}
\min_{\beta} (y-X\beta)'(y-X\beta)+\lambda \sum_{j=1}^{k}|\beta_j|
\end{equation}

LASSO пытается минимизировать взвешенную сумму $RSS=(y-X\beta)'(y-X\beta)$ и <<размера>> коэффициентов $\sum_{j=1}^{k}|\beta_j|$.


Мы не будем вдаваться в численные алгоритмы, которые используются при решении этой задачи.


Ridge regression отличается от LASSO ограничением $\sum \beta_j^2\leq c$. 
Также как и LASSO Ridge regression допускает альтернативную формулировку:

\begin{equation}
\min_{\beta} (y-X\beta)'(y-X\beta)+\lambda \sum_{j=1}^{k} \beta_j^2
\end{equation}

Также как и LASSO Ridge regression тоже приближает значения коэффициентов $\beta_j$ к нулю. 
Принципиальное отличие LASSO и RR. 
В LASSO краевое решение с несколькими коэффициентами равными нулю является типичной ситуацией. 
В RR коэффициент $\beta_j$ может оказаться точно равным нулю только по чистой случайности. 


LASSO допускает байесовскую интерпретацию...

Предположим, что априорное распределение параметров следующее:

...


Тогда мода апостериорного распределения будут приходится в точности (?) на оценки LASSO.


\todo[inline]{Может ли появиться мультимодальность? В точности ли на моду или только примерно?}


\section{Устоявшиеся слова}

Просьба <<проверьте гипотезу о значимости коэффициента>> на самом деле означает <<проверьте гипотезу о незначимости коэффициенты>>, т.к. проверяется $H_0$: $\beta_j=0$.

Просьба <<проверьте гипотезу о значимости регрессии в целом>> на самом деле означает <<проверьте гипотезу о незначимости регрессии в целом>>, т.к. проверяется $H_0$: $\beta_2=\ldots=\beta_k=0$.



\section{Pooled, Fixed and Random effect}

Здесь основная проблема в том, что часто путают описание модели и способ оценивания.


Модель Fixed effect


Способ оценивания Fixed effect

Будет состоятелен даже если на самом деле индивидуальные эффекты случайны! Даже если они коррелированы с регрессорами! (проверить русскую терминологию)







\end{document}