[
["index.html", "Конспект семинаров по метрике-2016 О конспекте", " Конспект семинаров по метрике-2016 Студенты ИП и Борис Демешев 2016-11-12 О конспекте Метрика с R :) Напутствия: Идеальный конспект — интересный и без отклонений от здравомыслия. Примочки и пеночки нужны! Конспект одного семинара должен иметь одно заглавие уровня # и несколько, скажем от двух до пяти, подзаголовков уровня ##. После «решёточек» должен идти пробел. После заголовка должен стоять краткий английский уникальный идентификатор, например, {#04_matrix_algebra}. Помни об оформлении знаков препинания: после запятой есть пробел, а до запятой — нет. Существует длинное тире, —, которое отличается от просто дефиса -. Рисунки оформляй в открытом софте (tex + tikz, inkscape, graphviz, geogebra, draw.io и прочее) и прикладывай к работе. Рисунки клади в подпапку images Соблюдай конвенцию о названиях файлов: файлы относящиеся к третьему семинару должны начинаться с 03_, и сам конспект и рисунки. Имена файлов не должны содержать русских букв и пробелов. Никаких здесь и тут в ссылках. Текст, замещающий ссылку, должен быть осмысленным! Немного про маркдаун. Немного про тех от Воронцова. Посмотри, как сделали конспект другие, и сделай лучше! :) Обрати внимание на название .Rmd файлов, на структуру внутри .Rmd файлов. Каждый кусок кода должен иметь уникальное название, например, {r, &quot;plotting_histogram&quot;} Уважай букву ё – ставь над ней точки! :) Тут всякие полезные для латеха: Данная версия конспекта скомпилирована для html. library(&quot;tidyverse&quot;) # ggplot2 for plots, dplyr for data manipulation, broom and more library(&quot;sandwich&quot;) # оценка Var для гетероскедастичности library(&quot;lmtest&quot;) # тест Бройша-Пагана library(&quot;data.table&quot;) # манипуляции с данными library(&quot;reshape2&quot;) # преобразование длинных таблиц в широкие "],
["ols-intro.html", "1 Метод наименьших квадратов 1.1 Основная задача 1.2 Реализация в R: 1.3 Домашнее задание", " 1 Метод наименьших квадратов Конспект: Бердникович Алеся, Головина Мария дата: 05.09.2016 1.1 Основная задача Маша каждый день ловит покемонов и решает задачи по теории вероятностей. Пусть x и y - случайные величины, \\(x_i\\) - количество решённых в i-тый день задач, а \\(y_i\\) - количество пойманных в i-тый день покемонов. Результаты наблюдения за действиями Маши представлены в таблице: День \\(x_i\\) \\(y_i\\) 1 1 10 2 2 0 3 0 4 Необходимо определить, как количество пойманных за день покемонов зависит от количества решённых за день задач. Предположим, что регрессионная модель имеет линейный вид \\(y_i = \\beta_1+\\beta_2 x_i + \\epsilon_i\\), где коэффициенты \\(\\beta_1, \\beta_2\\) неизвестны и должны быть оценены, а \\(\\epsilon_i\\) - случайная величина. Тогда прогнозируемая зависимость имеет вид \\(\\hat{y}_i=\\hat{\\beta}_1+\\hat{\\beta}_2x_i\\). 1.1.1 Метод наименьших квадратов (OLS): \\(y_i-\\hat{y}_i\\) - ошибка прогноза, которую нужно минимизировать. Штрафная функция: \\[ Q(\\hat{\\beta}_1,\\hat{\\beta}_2) = (y_1-\\hat{y}_1)^2 + (y_2-\\hat{y}_2)^2 + (y_3-\\hat{y}_3)^2 = (y_1-(\\hat{\\beta}_1+\\hat{\\beta}_2 x_1))^2 + (y_2-(\\hat{\\beta}_1+\\hat{\\beta}_2x_2))^2 + (y_3-(\\hat{\\beta}_1+\\hat{\\beta}_2x_3))^2 \\to min_{\\hat{\\beta}_1,\\hat{\\beta}_2} \\] 1.1.2 Метод наименьших модулей (LAD): Альтернативный метод минимизации ошибок прогноза. Отличие заключается в виде штрафной функции: \\[Q(\\hat{\\beta}_1,\\hat{\\beta}_2) = |y_1-\\hat{y_1}|^2 + |y_2-\\hat{y}_2|^2 + |y_3-\\hat{y}_3|^2 \\to min_{\\hat{\\beta}_1,\\hat{\\beta}_2}\\] Найдём \\(\\hat{\\beta}_1,\\hat{\\beta}_2\\) в нашей задаче методом наименьших квадратов: \\[Q(\\hat{\\beta}_1,\\hat{\\beta}_2) = (y_1-(\\hat{\\beta}_1+\\hat{\\beta}_2x_1)^2 + (y_2-(\\hat{\\beta}_1+\\hat{\\beta}_2x_2))^2 + (y_3-(\\hat{\\beta}_1+\\hat{\\beta}_2x_3))^2\\] \\[Q(\\hat{\\beta}_1,\\hat{\\beta}_2) = (10-(\\hat{\\beta}_1+\\hat{\\beta}_2)^2 + (0-(\\hat{\\beta}_1+2\\hat{\\beta}_2))^2 + (4-(\\hat{\\beta}_1))^2 \\to min\\] \\[\\frac{\\partial Q}{\\partial \\hat{\\beta}_1} = -28 + 6\\hat{\\beta}_1 + 6\\hat{\\beta}_2 = 0\\] \\[\\frac{\\partial Q}{\\partial \\hat{\\beta}_2} = -20 + 6\\hat{\\beta}_1 + 10\\hat{\\beta}_2 = 0\\] \\[\\hat{\\beta}_1 = \\frac{20}{3}, \\hat{\\beta}_2 = -2\\] Искомая оценка зависимости числа пойманных покемонов от числа решённых задач: \\[\\hat{y}_i = \\frac{20}{3} - 2x_i\\] 1.2 Реализация в R: x &lt;- c(1, 2, 0) y &lt;- c(10, 0, 4) md &lt;- data.frame(problem = x, pokemon = y) md ## problem pokemon ## 1 1 10 ## 2 2 0 ## 3 0 4 Восстановление линейной зависимости методом наименьших квадратов: model_1_ols &lt;- lm(data = md, pokemon~problem) summary(model_1_ols) ## ## Call: ## lm(formula = pokemon ~ problem, data = md) ## ## Residuals: ## 1 2 3 ## 5.333 -2.667 -2.667 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.667 5.963 1.118 0.465 ## problem -2.000 4.619 -0.433 0.740 ## ## Residual standard error: 6.532 on 1 degrees of freedom ## Multiple R-squared: 0.1579, Adjusted R-squared: -0.6842 ## F-statistic: 0.1875 on 1 and 1 DF, p-value: 0.7399 Подключаем нужный пакет: library(&quot;quantreg&quot;) Если пакета не установлен, то это исправляется командой install.packages(&quot;quantreg&quot;) Восстановление линейной зависимости методом наименьших модулей: model_1_lad &lt;- rq(data = md, pokemon~problem) summary(model_1_lad) ## ## Call: rq(formula = pokemon ~ problem, data = md) ## ## tau: [1] 0.5 ## ## Coefficients: ## coefficients lower bd upper bd ## (Intercept) 4.000000e+00 -1.797693e+308 1.797693e+308 ## problem -2.000000e+00 -1.797693e+308 1.797693e+308 Предположим теперь иную модель зависимости \\(y_i = \\hat{\\beta}x_i\\), ищем оценку единственного неизвестного коэффициента \\(\\hat{\\beta}\\) с помощью метода наименьших модулей. Штрафная функция примет вид \\[Q(\\hat{\\beta}) = |10-\\hat{\\beta}| + |0-\\hat{\\beta}| + |4-0| \\to min\\] Точки изломов функции находятся в нулях подмодульных выражений: \\(\\hat{\\beta}=0\\) и \\(\\hat{\\beta}=10\\). Функция принимает наименьшее значение при \\(\\hat{\\beta}=0\\) (см. график), что говорит об отсутствии зависимости числа пойманных покемонов от числа решённых задач. 1.2.1 График штрафной функции: x &lt;- seq(-10, 20, 0.001) fx &lt;- (x &lt;= 0) * (14 - 3 * x) + (x &gt; 0 &amp; x &lt; 10) * (14 + x) + (x &gt;= 10) * (3*x - 6) plot(x = x, y = fx, xlab = expression(hat(beta)), ylab = &#39;Q&#39;, pch = 20, col = &#39;orchid4&#39;) 1.3 Домашнее задание Вывести общие формулы для коэффициентов \\(\\hat{\\beta}, \\hat{\\beta}_1, \\hat{\\beta}_2\\), используя МНК-оценку, при условии, что: \\(y_i = \\beta x_i + \\epsilon_i, \\ \\hat{y}_i = \\hat{\\beta} x_i\\); \\(y_i = \\hat{\\beta}_1 + \\hat{\\beta}_2 x_i + \\epsilon_i, \\ \\hat{y}_i = \\hat{\\beta}_1+\\hat{\\beta}_2 x_i + \\epsilon_i\\). "],
["recall-all.html", "2 Вспомнить всё", " 2 Вспомнить всё Найдите длины векторов \\(a=(1,1,1)\\) и \\(b=(1,2,3)\\) и косинус угла между ними. Найдите один любой вектор, перпенидкулярный вектору \\(b\\). Сформулируйте теорему о трёх перпендикулярах и обратную к ней На плоскости \\(\\alpha\\) лежит прямая \\(\\ell\\). Вне плоскости \\(\\alpha\\) лежит точка \\(C\\). Ромео проецирует точку \\(C\\) на прямую \\(\\ell\\) и получает точку \\(R\\). Джульетта проецирует точку \\(C\\) сначала на плоскость \\(\\alpha\\), а затем проецирует полученную точку \\(A\\) на прямую \\(\\ell\\). После двух действий Джульетта получает точку \\(D\\). Обязательно ли \\(R\\) и \\(D\\) совпадают? Для матрицы \\[ A=\\begin{pmatrix} 5 &amp; 4 \\\\ 4 &amp; 5 \\\\ \\end{pmatrix} \\] Найдите собственные числа и собственные векторы матрицы Найдите \\(\\det (A)\\), \\({\\mathrm{tr}}(A)\\) Найдите собственные числа матрицы \\(A^{2016}\\), \\(\\det (A^{2016})\\) и \\({\\mathrm{tr}}(A^{2016})\\) Известно, что \\(X\\) — матрица размера \\(n \\times k\\) и \\(n&gt;k\\), известно, что \\(X&#39;X\\) обратима. Рассмотрим матрицу \\(H=X(X&#39;X)^{-1}X&#39;\\). Укажите размер матрицы \\(H\\), найдите \\(H^{2016}\\), \\({\\mathrm{tr}}(H)\\), \\(\\det(H)\\), собственные числа матрицы \\(H\\). Штрих означает транспонирование. Занудная халява: известно, что \\({\\mathbb{C}ov}(X, Y)=5\\), \\({\\mathbb{V}ar}(X)=10\\), \\({\\mathbb{V}ar}(Y)=20\\), \\({\\mathbb{E}}(X)=10\\), \\({\\mathbb{E}}(Y)=-10\\). Найдите \\({\\mathbb{C}ov}(X+2Y, Y-X)\\), \\({\\mathbb{V}ar}(X+2Y)\\), \\({\\mathbb{E}}(X+2Y)\\). За 100 дней Ромео посчитал все глубокие вздохи Джульетты. Настроение Джульетты столь спонтанно, что глубокие вздохи за разные дни можно считать независимыми. В сумме оказалось 890 вздохов. Сумма квадратов оказалась равна 8000. Постройте 95%-ый доверительный интервал для математического ожидания ежедневного количества глубоких вздохов Джульетты. На~уровне значимости 5%-ов проверьте гипотезу, что математическое ожидание равно~9. Ромео подкидывает монетку два раза. Если монетка выпадает орлом, то Ромео кладет в мешок черный шар, если решкой — белый. Джульетта не знает, как выпадала монетка, и достает шары из мешка наугад по очереди. Первый шар оказался черного цвета. Какова вероятность того, что второй шар Джульетты будет белым? Что-то с памятью моей стало… Линейная алгебра: Великолепный учебник! Strang, Introduction to linear algebra. Ознакомиться можно на gen.lib.rus.ec :) "],
["ols-geometry.html", "3 Геометрия МНК 3.1 Обозначения 3.2 Ныряем в \\(n\\)-мерное пространство 3.3 Больше проекций", " 3 Геометрия МНК конспект: Света Колесниченко дата: 19 сентября 2016 3.1 Обозначения Варианты представления регрессии: Скалярный вариант: \\(\\hat y_{i} = \\hat \\beta_1 + \\hat \\beta_2\\, x_{i} + \\hat \\beta_3\\, z_{i}\\) Векторный вариант: \\(\\hat y = \\hat \\beta_1\\, e + \\hat \\beta_2\\, x + \\hat \\beta_3\\, z\\) \\[ \\begin{matrix} &amp; e = \\vec 1 = \\\\ \\end{matrix} \\begin{pmatrix} 1 \\\\ \\vdots \\\\ 1 \\end{pmatrix} \\begin{matrix} \\textit{ - единичный вектор размерности $n \\times 1$}, \\end{matrix} \\] \\[ \\begin{matrix} &amp; x = \\\\ \\end{matrix} \\begin{pmatrix} x_{1} \\\\ \\vdots \\\\ x_{n} \\end{pmatrix} \\begin{matrix} &amp; y = \\\\ \\end{matrix} \\begin{pmatrix} y_{1} \\\\ \\vdots \\\\ y_{n} \\end{pmatrix} \\begin{matrix} &amp; z = \\\\ \\end{matrix} \\begin{pmatrix} z_{1} \\\\ \\vdots \\\\ z_{n} \\end{pmatrix} \\textit{ - векторы переменных} \\] Количество наблюдений = \\(n\\), количество коэффициентов \\(\\beta\\) = количество регрессоров = \\(k\\). Матричный вариант: \\(\\hat y = X\\, \\hat \\beta\\) \\[ \\begin{matrix} &amp; \\hat \\beta = \\\\ \\end{matrix} \\begin{pmatrix} \\hat \\beta_{1} \\\\ \\vdots \\\\ \\hat \\beta_{k} \\end{pmatrix} \\begin{matrix} \\textit{ - вектор размера } k\\times 1, \\end{matrix} \\] \\[ \\begin{matrix} &amp; X = \\\\ \\end{matrix} \\begin{pmatrix} 1 &amp; x_{1} &amp; z_{1} \\\\ \\vdots &amp; \\vdots &amp; \\vdots \\\\ 1 &amp; x_n &amp; z_{n} \\end{pmatrix} \\begin{matrix} \\textit{ - матрица размера } n \\times k \\end{matrix} \\] Конвенция об обозначениях: \\(y, \\beta, \\hat \\beta, x, z\\) - векторы \\(y_{i}, \\beta_{j}, \\hat \\beta_{7}, x_{45}, z_{37}\\) - числа (скаляры) \\(\\Omega, X, H\\) - матрицы 3.2 Ныряем в \\(n\\)-мерное пространство \\[ \\min_{i\\in I} \\sum_{i=1}^n (y_{i} - \\hat y_{i})^{2} = \\min_{i\\in I} \\sum_{i=1}^n |y_{i} - \\hat y_{i}|^{2} \\textit{ - минимизируем квадрат длины вектора} \\] \\[ \\begin{pmatrix} \\bar y \\\\ \\vdots \\\\ \\bar y \\end{pmatrix} \\begin{matrix} &amp; = \\bar y \\cdot \\\\ \\end{matrix} \\begin{pmatrix} 1 \\\\ \\vdots \\\\ 1 \\end{pmatrix} \\begin{matrix} &amp; = \\bar y \\cdot \\vec 1 \\end{matrix} \\] \\(\\bar y = \\hat y\\) - т.к. \\(\\bar y \\cdot \\vec 1 = \\hat y \\cdot \\vec 1\\) среднее значение = среднее значение прогнозов (\\(\\hat y_{i}\\)) Картиночка Лапы “Лапа” = \\(Lin (e, x, z) \\leftarrow\\) выбираем через e, x, z положение \\(\\hat y\\) \\(\\hat y\\) - проекция y на “лапу” y - линейная комбинация e, x, z \\(\\rightarrow\\) лежит в линейной оболочке этих векторов \\(\\hat \\varepsilon = y - \\hat y\\) - вектор “остатков”/ошибок прогнозов/resideals \\(\\hat \\varepsilon \\,\\bot\\, e, \\hat \\varepsilon \\,\\bot\\, x, \\hat \\varepsilon \\,\\bot\\, y\\) \\(\\hat \\varepsilon \\cdot \\vec 1 = 0\\), \\(\\hat \\varepsilon \\cdot x = 0\\), \\(\\hat \\varepsilon \\cdot z = 0\\) \\(\\leftarrow\\) скалярное произведение векторов (ссыль подробнее) перпендикулярных векторов равно 0. \\(\\hat \\varepsilon \\, \\bot \\, \\textit{Лапа} \\rightarrow \\hat \\varepsilon \\, \\bot \\, \\textit{любому вектору, лежащему в Лапе}\\) Великая Теорема о 3 перпендикулярах и аж в 2 формулировках и с чертёжиком. \\[ \\sum_{i=1}^n \\hat \\varepsilon_{i}=0 , \\sum_{i=1}^n \\hat \\varepsilon_{i} x_{i}=0 \\] \\[ \\begin{matrix} &amp; X&#39; = \\\\ \\end{matrix} \\begin{pmatrix} 1 &amp; \\dots &amp; 1\\\\ x &amp; \\dots &amp; x_{n} \\\\ z &amp; \\dots &amp; z_{n} \\end{pmatrix} \\begin{matrix} \\textit{ - единичный вектор размерности $n \\times 1$} \\end{matrix} \\begin{matrix} &amp; \\hat \\varepsilon = \\\\ \\end{matrix} \\begin{pmatrix} \\hat \\varepsilon_{1} \\\\ \\vdots \\\\ \\hat \\varepsilon_{n} \\end{pmatrix} \\begin{matrix} \\textit{ - единичный вектор размерности $n \\times 1$} \\end{matrix} \\] Условие ортогональности: \\(X&#39; \\cdot \\hat \\varepsilon = 0\\) - размерность этого нуля - \\(k \\times 1\\) \\(\\hat y = X \\cdot \\hat \\beta \\rightarrow \\hat \\beta = \\frac{\\hat y}{X} = \\frac{\\hat \\varepsilon - Y}{X}\\) 3.2.1 Упражнение 1 Выведите \\(\\hat \\beta\\) из \\(X&#39;\\cdot(y-X\\cdot\\hat \\beta) = 0\\) \\(\\hat \\varepsilon = y - \\hat y = y - X\\cdot\\hat \\beta\\) \\(X\\) - задает “лапу”. \\(\\hat \\beta\\) - отвечает за то, с каким весом в \\(\\hat y\\) входят базисные векторы «лапы». \\(\\hat \\beta = (\\sum_{i=1}^n x^2_{i})^{-1}\\, \\sum_{i=1}^n x_{i} y_{i}=0\\) - для \\(\\hat y_{i} = \\hat \\beta \\, x_{i}\\) \\(X&#39;\\cdot y = X&#39;\\cdot X\\cdot \\hat \\beta\\) \\((X&#39; \\cdot X)^{-1}\\cdot X&#39; \\cdot y = (X&#39; \\cdot X)^{-1}\\cdot X&#39;\\cdot X \\cdot \\hat \\beta\\) \\(\\hat \\beta = (X&#39; \\cdot X)^{-1}\\cdot X&#39;\\cdot y\\) 3.3 Больше проекций 3.3.1 Упражнение 2 Спроецируйте вектор \\(\\,\\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\end{pmatrix}\\,\\) на прямую, порождённую вектором \\(\\,\\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{pmatrix}\\,.\\) Визуализация задачи \\[ \\begin{matrix} &amp; \\hat \\beta = \\\\ \\end{matrix} \\begin{pmatrix} 1 &amp; 1 &amp; 1 &amp; 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{pmatrix} \\begin{pmatrix} 1 &amp; 1 &amp; 1 &amp; 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\end{pmatrix} \\begin{matrix} = 4^{-1} \\cdot 10 \\cdot \\frac{1}{4} \\cdot 10 = 2.5 \\\\ \\end{matrix} \\] \\[ \\begin{matrix} &amp; \\hat y = X \\cdot \\hat \\beta = \\\\ \\end{matrix} \\begin{pmatrix} 2.5 \\\\ 2.5 \\\\ 2.5 \\\\ 2.5 \\end{pmatrix} \\begin{matrix} = \\\\ \\end{matrix} \\begin{pmatrix} \\bar y \\\\ \\bar y \\\\ \\bar y \\\\ \\bar y \\end{pmatrix} \\] \\[ \\bar y = \\frac{1+2+3+4}{4} = 2.5 \\] Проекция вектора на прямую из единиц даёт вектор из средних. By the way, крутые читщиты по матрицам и основам линейной алгебры. 3.3.2 Упражнение 3 Сформулируйте все теоремы Пифагора \\(\\{\\hat \\varepsilon, \\hat y - \\bar y \\cdot \\vec 1, y - \\bar y \\cdot \\vec 1\\}\\,\\) Чертёжик одного из треугольников По Теореме Пифагора: \\[ |y - \\bar y \\cdot \\vec 1|^{2} = |\\hat \\varepsilon|^2 + |\\hat y - \\bar y \\cdot \\vec 1|^2 \\] \\[ \\sum_{i=1}^n (y_{i}- \\bar y)^2 = \\sum_{i=1}^n \\hat \\varepsilon^2_{i} + \\sum_{i=1}^n (\\hat y_{i}- \\bar y)^2 \\] \\[ \\begin{matrix} &amp; y_{i}- \\bar y = \\\\ \\end{matrix} \\begin{pmatrix} y_{1} - \\bar y\\\\ \\vdots \\\\ y_{n} - \\bar y \\end{pmatrix} \\] Полное задание см. в Задачнике по координатам: 4.23, 4.24, 4.25 Коэффициент детерминации (\\(R^2\\)) - примитивный показатель качества прогнозов. \\[ R^2 = \\frac{ESS}{TSS} = \\frac{\\sum_{i=1}^n \\hat \\varepsilon^2_{i}}{\\sum_{i=1}^n (y_{i}- \\bar y)^2} = \\frac{\\sum_{i=1}^n (y_{i}- \\hat y)^2}{\\sum_{i=1}^n (y_{i}- \\bar y)^2} = \\frac{\\text{residial sum of squares (cумма квадратов остатков)}}{\\text{total sum of squares (полная сумма квадратов)}} \\] \\[ ESS = \\sum_{i=1}^n (\\hat y_{i}- \\bar y)^2 = \\, \\text{explained sum of squares (&quot;объясненная&quot; сумма квадратов)} \\] В МНК работает (и точно только в нём!) соотношение: \\(RSS + ESS = TSS\\). В МНК решается задача минимизации RSS. Если прогнозы \\(\\hat y_i\\) идеально совпадают с \\(y_i\\), то \\(R^2 = 1 \\Rightarrow ESS = TSS\\). \\(R^2 \\in [0;1]\\, , R^2 = \\cos^2 \\rho\\) \\(\\hat y\\) ближе к \\(y\\) с ростом «лапы» \\(\\Rightarrow\\) \\(\\angle \\rho \\downarrow \\, \\rightarrow R^2\\,\\) т.к. \\(\\cos^2 \\rho \\uparrow\\) 3.3.2.1 ДЗ: 1.1, 1.2, 1.7, 1.12, 1.13, 4.13 (1-6), 4.23, 4.24, 4.25 из Задачника 3.3.2.1.1 Полезные ссылки: Репозиторий курса метрики и теории вероятностей "],
["matrix-fight.html", "4 Борьба с матрицами", " 4 Борьба с матрицами дата: 26 сентября 2016 конспект: Вика Шрамова, Эдуард Аюнц Семинар посвящен работе с матрицами - матричному дифференцированию и представлению многомерных случайных величин при помощи матриц. Перед тем как приступить к работе с матрицами, полезно повторить основные свойства операций над матрицами: \\(A(B+C) = AB+ AC\\) \\((A+B)^T=A^T + B^T\\) \\((AB)^T = B^T A^T\\) \\((AB)^{-1} = B^{-1} A^{-1}\\) \\((A^{-1})^T = (A^T)^{-1}\\) Производные следа и определителя: \\(tr(AB)&#39;_A = B^T\\) \\(det(A)&#39;_A = det(A) (A^{-1})^T\\) \\((log det A(x) )&#39;_x = tr(A^{-1} A&#39;_x)\\) След и определитель: \\(det(AB) = det(A) det(B)\\) \\(det(A^{-1})= 1/det(A)\\) \\(det(A) = \\prod_j \\lambda_j\\) \\(tr(A) = \\sum_j A_{jj} = \\sum_j \\lambda_j\\) \\(tr(ABC) = tr(BCA) = tr(CAB)\\) Для начала напомним о разнице между одномерными и многомерными случайными величинами. Обозначим \\(y\\) как случайный вектор \\(\\left( \\begin{matrix} y_1 \\\\ \\vdots \\\\ y_n \\end{matrix} \\right)\\). Одномерную случайную величину будем обозначать маленькими латинскими буквами с индексами: \\(y_1\\). \\(Var(y)\\) = \\(\\left( \\begin{matrix} Var(y_1) &amp; Cov (y_1,_2) &amp; \\hdots Cov (y_1,_n) \\\\ \\vdots &amp; \\vdots &amp; \\vdots \\\\ Cov(y_k,y_1) &amp; Var (y_k) &amp; \\hdots Cov (y_k,y_n) \\\\ \\vdots &amp; \\vdots &amp; \\vdots \\\\ Cov(y_n,y_1) &amp; Cov (y_n,_2) &amp; \\hdots Var (y_n) \\end{matrix} \\right)\\) Из такой записи ковариции векторов очевидно, что если в формуле ковариации поменять местами векторы, то их матрица ковариации будет являться транспонированной матрицой ковариации векторов в исходной последовательности. \\(Cov(y,z) =Cov (z,y)ˆ{T}\\) Упражнение Дана матрица \\(A = \\left( \\begin{matrix} 2 &amp; 0 \\\\ 1 &amp; 3\\\\ \\end{matrix} \\right)\\) и случайный вектор \\(y = \\left( \\begin{matrix} y_1 \\\\ y_2 \\end{matrix} \\right)\\) с матожиданием \\(E(y) = \\left( \\begin{matrix} 2 \\\\ 7 \\end{matrix} \\right)\\) и дисперсией \\(Var(y) = \\left( \\begin{matrix} 3 &amp; 1 \\\\ 1 &amp; 1 \\end{matrix} \\right)\\) Требуется найти \\(E(z), Var(z), Cov(y,z).\\) Решение \\(E(z) = A \\cdot E(y) = \\left( \\begin{matrix} 2 &amp; 0 \\\\ 1 &amp; 3\\\\ \\end{matrix} \\right)\\) \\(\\cdot\\) \\(\\left( \\begin{matrix} 2 \\\\ 7 \\end{matrix} \\right)\\) = \\(\\left( \\begin{matrix} 4 \\\\ 23 \\end{matrix} \\right)\\) \\(Var(z)\\) = \\(A \\cdot Var(y) \\cdot Aˆ{T}\\) = \\(\\left( \\begin{matrix} 2 &amp; 0 \\\\ 1 &amp; 3\\\\ \\end{matrix} \\right)\\) \\(\\cdot\\) \\(\\left( \\begin{matrix} 3 &amp; 1 \\\\ 1 &amp; 7\\\\ \\end{matrix} \\right)\\) \\(\\cdot\\) \\(\\left( \\begin{matrix} 2 &amp; 1 \\\\ 0 &amp; 3\\\\ \\end{matrix} \\right)\\) = \\(\\left( \\begin{matrix} 12 &amp; 12 \\\\ 12 &amp; 72\\\\ \\end{matrix} \\right)\\) \\(t = Ay\\) = \\(\\left( \\begin{matrix} 2y_1 \\\\ y_1 +3y_2 \\end{matrix} \\right)\\) \\(Cov(y,z)\\) = \\(\\left( \\begin{matrix} Cov (y_1, z_1) &amp; Cov (y_1,z_2) \\\\ Cov(y_2,z_1) &amp; Cov (y_1, z_2) \\end{matrix} \\right)\\) = \\(Cov(y, Ay) = Cov(y,y) Aˆ{T}\\) = \\(\\left( \\begin{matrix} 3 &amp; 1 \\\\ 1 &amp; 7\\\\ \\end{matrix} \\right)\\) \\(\\cdot\\) \\(\\left( \\begin{matrix} 2 &amp; 1 \\\\ 0 &amp; 3\\\\ \\end{matrix} \\right)\\) Упражнение Предположим, существует истинная зависимость \\(y = X\\beta +\\varepsilon\\) между оцениваемыми величиными. При оценивании параметров модели МНК будут фигурировать следующие величины: \\(y, \\hat{y}, \\varepsilon, \\hat{\\varepsilon}, \\beta , \\hat{\\beta}\\). Оценим все матожидания, дисперсии и ковариации указанных величин. Перед тем, как начать, необходимо сделать оговорку, что существуют две парадигмы исследования: 1) Предполагается, что матрица X является детерминированной. 2) Матрица X состоит из случайных величин. В ходе нашего курса мы будем работать со случайными X, однако пока будем считать, что матрица Х детерминирована. Решение Оценка \\(\\hat{\\beta} = (X&#39;X)^{T} Xy\\), \\(\\hat{\\varepsilon} = y- \\hat{y}\\), \\(\\hat{y} =X\\hat{\\beta}\\) Найдем матожидания: \\(E(\\beta) = beta,\\) так как \\(\\beta\\) – вектор неизвестных констант \\(E(\\epsilon) = 0\\) (по предпосылкам МНК) \\(E(y) = E(X\\beta + \\epsilon) = XE(\\beta) + E(\\epsilon) = X\\beta\\) \\(E(\\hat{y}) = E(X\\hat{\\beta}) = XE(\\hat{\\beta}) = XE((X^{&#39;}X)^{-1}X^{&#39;}y) = X(X^{&#39;}X)^{-1}X^{&#39;}E(y) = X(X^{&#39;}X)^{-1}X^{&#39;}X\\beta = X\\beta\\) \\(E(\\hat{\\epsilon}) = E(y - \\hat{y}) = E(y) - E(\\hat{y}) = X\\beta - X\\beta = 0\\) \\(E(\\hat{\\beta}) = (X^{&#39;}X)^{-1}X^{&#39;}E(y) = (X^{&#39;}X)^{-1}X^{&#39;}X\\beta = \\beta\\) Найдем, к примеру, \\(Cov(\\epsilon, \\hat{\\beta})\\): \\(Cov(\\epsilon,\\hat{\\beta}) = Cov(\\epsilon,(X^{&#39;}X)^{-1}X^{&#39;}y) = Cov(\\epsilon,(X^{&#39;}X)^{-1}X^{&#39;}(X\\beta+\\epsilon)) = Cov(\\epsilon,(X^{&#39;}X)^{-1}X^{&#39;}X\\beta+(X^{&#39;}X)^{-1}X^{&#39;}\\epsilon) = Cov(\\epsilon,\\epsilon)((X^{&#39;}X)^{-1}X^{&#39;})^{&#39;} = \\sigma^{2}I((X^{&#39;}X)^{-1}X^{&#39;})^{&#39;} = \\sigma^{2}X^{&#39;&#39;}((X^{&#39;}X)^{-1})^{&#39;} = \\sigma^{2}X((X^{&#39;}X)^{&#39;})^{-1} = \\sigma^{2}X(X^{&#39;}X)^{-1}\\) Существует 2 традиции матричного дифференцирования, суть различия которых заключается в представлении вектора (матрицы) производной — в виде столбца или в виде строки. Основные различия представлены в следующей таблице: \\[\\begin{align} \\frac{\\partial f}{\\partial x} &amp;= \\begin{pmatrix} \\dfrac{\\partial f}{\\partial x_{1}} \\\\ \\dfrac{\\partial f}{\\partial x_{2}} \\\\ \\vdots \\\\ \\dfrac{\\partial f}{\\partial x_{n}} \\end{pmatrix} \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; \\frac{\\partial f}{\\partial X} &amp;= \\begin{pmatrix} \\dfrac{\\partial f}{\\partial x_{11}} &amp; \\cdots &amp; \\dfrac{\\partial f}{\\partial x_{1k}} \\\\ \\vdots &amp; &amp; \\vdots \\\\ \\dfrac{\\partial f}{\\partial x_{n1}} &amp; \\cdots &amp; \\dfrac{\\partial f}{\\partial x_{nk}} \\end{pmatrix} \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; \\frac{\\partial g}{\\partial x} &amp;= \\begin{pmatrix} \\dfrac{\\partial g_{1}}{\\partial x_{1}} &amp; \\cdots &amp; \\dfrac{\\partial g_{k}}{\\partial x_{1}} \\\\ \\vdots &amp; &amp; \\vdots \\\\ \\dfrac{\\partial g_{1}}{\\partial x_{n}} &amp; \\cdots &amp; \\dfrac{\\partial g_{k}}{\\partial x_{n}} \\end{pmatrix} \\end{align}\\] Если \\(x\\) – вектор, \\(f(x) = Ax\\), то \\(\\dfrac{\\partial f}{\\partial x} = A^{&#39;}\\) Если \\(f(x) = x^{&#39;}Ax\\), то \\(\\dfrac{\\partial f}{\\partial x} = (A + A^{&#39;})x\\) Если \\(f(X) = det(X)\\), то \\(\\dfrac{\\partial f}{\\partial x} =det(X)(X^{-1})^{&#39;}\\) Доказательство второго свойства и не только можно почитать здесь "],
["-05-models-evaluation.html", "5 Доказательство свойств и оценка моделей. {#05_models_evaluation}", " 5 Доказательство свойств и оценка моделей. {#05_models_evaluation} конспект: Артём Калинин, Кирилл Улыбин дата: 9 сентября 2016 5.0.1 Пример из домашки Сначала разобрали пример из домашки прошлого семинара: Искали \\(cov(\\hat{y}, \\hat{\\epsilon})=?\\) \\(cov(\\hat{y}, \\hat{\\epsilon})= cov(X\\hat{\\beta}, y - \\hat{y})=cov(y-\\hat{y}, X\\hat{\\beta})&#39;=cov((I-\\underbrace{X(X&#39;X)^{-1}X&#39;}_H)y , X(X&#39;X)^{-1}X&#39;y))&#39;=\\) \\(=((I-H)cov(y, y)(H)&#39;)&#39;=(\\sigma^{2}(IH-H))&#39;=(\\sigma^{2}(H-H))&#39;=0\\) Интуитивное объяснение результата - предсказания не должны зависеть от ошибок. Например, если бы предсказания положительно зависели от ошибок, можно было бы сделать поправку в предсказании на известную величину ошибки. Вспомогательно: H - матрица-шляпница! Почему? H*(любой вектор)=(его проекция) \\(Hy=\\hat{y}\\) \\(H&#39;=H\\) \\(H^{2}=X(X&#39;X)^{-1}\\underbrace{X&#39;X(X&#39;X)^{-1}}_IX&#39;=H\\) 5.0.2 Упражнение 1 Даны модель А: \\(y_i=\\beta x_i + \\epsilon_i\\), модель B: \\(y_i = \\beta_1+\\beta_2 x_i + \\epsilon_i\\) \\(\\sum_{i=1}^n x_i=50\\);\\(\\sum_{i=1}^n x_iy_i=-50\\); \\(\\sum_{i=1}^n x_i^2=2000\\); \\(\\sum_{i=1}^n y_i=20\\); \\(\\sum_{i=1}^n y_i^2=500\\); \\(n=100\\) Найти для модели B (для модели A - дома): \\(1)X-?\\) \\(2)X&#39;X-?\\) \\(3)\\hat{\\beta}-?\\) \\(4)Var(\\hat{\\beta})-?\\) \\(5)X&#39;y-?\\) Решение 1)\\(X\\) = \\(\\left( \\begin{matrix} 1 &amp; x_1 \\\\ 1 &amp; x_2 \\\\ \\vdots &amp; \\vdots \\\\ 1 &amp; x_n\\\\ \\end{matrix} \\right)\\) 2)\\(X&#39;\\) = \\(\\left( \\begin{matrix} 1 &amp; 1 &amp; \\dots &amp; 1 \\\\ x_1 &amp; x_2 &amp; \\dots &amp; x_n \\\\ \\end{matrix} \\right)\\) \\((X&#39;X)\\) = \\(\\left( \\begin{matrix} 1 &amp; 1 &amp; \\dots &amp; 1 \\\\ x_1 &amp; x_2 &amp; \\dots &amp; x_n \\\\ \\end{matrix} \\right)\\) \\(\\left( \\begin{matrix} 1 &amp; x_1 \\\\ 1 &amp; x_2 \\\\ \\vdots &amp; \\vdots \\\\ 1 &amp; x_n\\\\ \\end{matrix} \\right)\\)=\\(\\left( \\begin{matrix} n &amp; x_1+x_2+...x_n &amp;\\\\ x_1+x_2+...+x_n &amp; x_1^2+x_2^2+...+x_n^2 &amp;\\\\ \\end{matrix} \\right)\\)=\\(\\left( \\begin{matrix} 100 &amp; 50 &amp;\\\\ 50 &amp; 2000 &amp;\\\\ \\end{matrix} \\right)\\) 3)\\(X&#39;y\\)=\\(\\left( \\begin{matrix} 1 &amp; \\dots &amp; 1 &amp;\\\\ x_1 &amp; \\dots &amp; x_n &amp;\\\\ \\end{matrix} \\right)\\) \\(\\left( \\begin{matrix} y_1&amp;\\\\ \\vdots&amp;\\\\ y_n&amp;\\\\ \\end{matrix} \\right)\\)=\\(\\left( \\begin{matrix} \\sum_{i=1}^n y_i &amp;\\\\ \\sum_{i=1}^n x_iy_i &amp;\\\\ \\end{matrix} \\right)\\)=\\(\\left( \\begin{matrix} 20 \\\\ -50 \\\\ \\end{matrix} \\right)\\) 4)\\(\\hat{\\beta}=(X&#39;X)^{-1}X&#39;y=\\left( \\begin{matrix} 100 &amp; 50\\\\ 50 &amp; 2000\\\\ \\end{matrix} \\right)^{-1}=\\left( \\begin{matrix} 20 \\\\ -50 \\\\ \\end{matrix} \\right)=\\frac{1}{197500}\\left( \\begin{matrix} 2000 &amp; -50\\\\ -50 &amp; 100\\\\ \\end{matrix} \\right)\\left( \\begin{matrix} 20 \\\\ -50 \\\\ \\end{matrix} \\right)=\\frac{1}{197500}\\left( \\begin{matrix} 42500 \\\\ -51000 \\\\ \\end{matrix} \\right)\\) 5)\\(Var(\\hat{\\beta})=\\sigma^2(X&#39;X)^{-1}=\\sigma^2*\\frac{1}{197500}\\left(\\begin{matrix} 2000 &amp; -50 \\\\ -50 &amp; 100\\\\ \\end{matrix} \\right)\\) 5.0.3 Упражнение 2 Доказать, что \\(Var(Ay)=AVar(y)A&#39;\\) Напомним, что \\(Var(z) = \\left(\\begin{matrix} var(z_1) &amp; cov(z1,z2) &amp; \\dots &amp; cov(z_1,z_n) \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots \\\\ cov(z_n, z_1) &amp;\\dots &amp;\\dots &amp; var(z_n)\\\\ \\end{matrix} \\right)\\) Сначала докажем вспомогательные утверждения: а)\\(E(Ay)=AE(y)\\) б)\\(E(zB)=E(z)B\\) в)\\(Var(z)=E(zz&#39;) - E(z)E(z&#39;)\\) а)\\(E(Ay)=AE(y)\\) Левая часть: \\(E(Ay)=Left_{ij} = E(\\sum_{k=1}^s a_{ik}y_{kj})=\\sum_{k=1}^s a_{ik}E(y_{kj})\\) Правая часть: \\(AE(y) = Right_{ij}=\\sum_{k=1}^s a_{ik}E(y_{kj})\\) б)\\(E(zB)=E(z)B\\); Док-во такое же как в (а) в)\\(Var(z)=E(zz&#39;) - E(z)E(z&#39;)\\) Левая часть: \\(Var(z) = Left_{ij}=cov(z_i,z_j)\\) Правая часть: \\(Right_{ij}=E(zz&#39;)_{ij} - (E(z)E(z&#39;))_{ij}=E(z_iz_j)-E(z_i)E(z_j)=cov(z_i,z_j)\\) \\(E(zz&#39;)= \\left( \\begin{matrix} E(z_1^2) &amp; E(z_1z_2) &amp; \\dots &amp; E(z_1z_n) \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots\\\\ E(z_nz_1) &amp; \\dots &amp; \\dots &amp; E(z_n^2)\\\\ \\end{matrix} \\right)\\) \\(E(z)E(z&#39;) = \\left( \\begin{matrix} E(z_1)E(z_1) &amp; \\dots &amp; E(z_1)E(z_n) \\\\ \\vdots &amp; \\vdots &amp; \\vdots \\\\ E(z_n)E(z_1) &amp; \\dots &amp; E(z_n)E(z_n)\\\\ \\end{matrix} \\right)\\) Теперь вернемся к доказательству \\(Var(Ay)=AVar(y)A&#39;\\). \\(Var(Ay) = E(Ay(Ay)&#39;)-E(Ay)E((Ay)&#39;)=E(Ayy&#39;A&#39;)-E(Ay)E(y&#39;A&#39;)=AE(yy&#39;)A&#39;-AE(y)E(y&#39;)A&#39;=A(\\underbrace{E(yy&#39;)-E(y)E(y&#39;))}_{Var(y)}A&#39;=AVar(y)A&#39;\\) 5.0.4 Упражнение 3 Дано \\(X_{n\\times1}\\), \\(A_{n\\times n}\\) \\(f(A)_{1\\times 1}=X&#39;_{1\\times n}A_{n\\times n}X_{n\\times 1}\\) Найти \\(\\frac{\\partial f}{\\partial A}\\), т.е. скаляр дифференцируем по каждому элементу матрицы А. Решение: \\(\\frac{\\partial f}{\\partial A}_{ij}=\\frac{\\partial f}{\\partial a_{ij}}\\) \\(X&#39;A= \\left( \\begin{matrix} x_1 &amp; x_2 &amp;\\dots &amp; x_n \\\\ \\end{matrix} \\right) \\left( \\begin{matrix} a_{11} &amp; a_{12}&amp;\\dots &amp; a_{1n} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots\\\\ a_{n1} &amp; \\dots &amp;\\dots &amp; a_{nn} \\end{matrix} \\right) = \\left( \\begin{matrix} \\sum_{i=1}^n x_ia_{i1} &amp; \\sum_{i=1}^n x_ia_{i2}&amp; \\dots &amp; \\sum_{i=1}^n x_ia_{in} \\\\ \\end{matrix} \\right)_{1\\times n}\\) \\(X&#39;AX = \\left( \\begin{matrix} \\sum_{i=1}^n x_ia_{i1} &amp; \\sum_{i=1}^n x_ia_{i2}&amp; \\dots &amp; \\sum_{i=1}^n x_ia_{in} \\\\ \\end{matrix} \\right)\\left( \\begin{matrix} x_1 \\\\ x_2\\\\ \\vdots\\\\ x_n\\\\ \\end{matrix} \\right)= \\sum_{j=1}^n (x_j \\sum_{i=1}^n x_i a_{ij})\\) Тогда можно переписать в виде: \\(f(A)= \\sum_{j=1}^n (x_j \\sum_{i=1}^n x_i a_{ij})= \\sum_{i=1, j=1}^n x_ix_ja_{ij}\\) То есть \\(\\frac{\\partial f}{\\partial A_{ij}}=x_ix_j\\) \\(\\frac{\\partial f}{\\partial A}= \\left( \\begin{matrix} x_1^2 &amp; x_1x_2 &amp; \\dots &amp; x_1x_n\\\\ x_2x_1&amp; x_2^2 &amp; \\dots &amp; x_2x_n \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots\\\\ x_nx_1&amp; \\dots &amp; \\dots &amp; x_n^2\\\\ \\end{matrix} \\right)=XX&#39;\\) 5.0.5 Упражнение 4. Оценим модель с помощью ML Пусть истинная зависимость \\(y = X\\beta +\\varepsilon\\), причем \\(\\varepsilon \\sim N(0;\\sigma^{2}I)\\) Всего n наблюдений и k регрессоров Найти: а) \\(\\hat{\\beta_{ML}}\\), \\(\\hat{\\sigma^{2}_{ML}}\\) б) \\(E(\\hat{\\beta_{ML}})\\), \\(E(\\hat{\\sigma}^{2}_{ML})\\) с) \\(Var(\\hat{\\beta_{ML}})\\), \\(Var(\\hat{\\sigma}^{2}_{ML})\\) Решение а) Найдем ML оценки X и y известны, оцениваем \\(\\beta = \\left( \\begin{matrix} \\beta_1 \\\\ \\vdots \\\\ \\beta_k \\end{matrix} \\right)\\) и \\(\\sigma^{2}\\) Из \\(\\varepsilon \\sim N(0;\\sigma^{2}I))\\) и \\(y = X\\beta +\\varepsilon\\) следует, что \\(y \\sim N(X\\beta;\\sigma^{2}I)\\) Запишем формулу плотности многомерного нормального распределения: \\(p(y)=\\frac{1}{\\sqrt{(2\\pi)^n} \\sqrt{det(\\sigma^{2}I)}}e^{-\\frac{1}{2}(y-X\\beta)^{&#39;}(\\sigma^{2}I)^{-1}(y-X\\beta)}\\) Для удобства логарифмируем и получим задачу максимизации фукции правдоподобия: \\(Q=ln(p(y))=-\\frac{n}{2}ln(2\\pi)-\\frac{1}{2}ln(det(\\sigma^{2}I))-\\frac{1}{2}(y-X\\beta)^{&#39;}(\\sigma^{2}I)^{-1}(y-X\\beta) \\rightarrow \\max\\limits_{\\beta, \\sigma^{2}}\\) Заметим, что первое слагаемое не влияет на решение задачи максимизации, а \\(det(\\sigma^{2}I)=\\sigma^{2n}\\) \\(\\frac{\\partial Q}{\\partial \\beta}\\): \\(Q=-\\frac{1}{2\\sigma^{2}}(y^{&#39;}-X^{&#39;}\\beta^{&#39;})(y-X\\beta)=-\\frac{1}{2\\sigma^{2}}(y^{&#39;}y-\\beta^{&#39;}X^{&#39;}y-y^{&#39;}X\\beta+\\beta^{&#39;}X^{&#39;}X\\beta)\\) Заметим: \\(y^{&#39;}X\\beta\\) - скаляр, причем \\((y^{&#39;}X\\beta)^{&#39;}=\\beta^{&#39;}X^{&#39;}y\\), тогда можно записать в виде: \\(Q=-\\frac{1}{2\\sigma^{2}}(y^{&#39;}y-2y^{&#39;}X\\beta+\\beta^{&#39;}X^{&#39;}X\\beta)\\) \\(\\frac{\\partial Q}{\\partial \\beta}=-\\frac{1}{2\\sigma^{2}}((-2y^{&#39;}X)^{&#39;}+(X^{&#39;}X+(X^{&#39;}X)^{-1})\\hat{\\beta})=0\\) \\(X^{&#39;}X\\hat{\\beta}=X^{&#39;}y\\) \\(\\hat{\\beta}=(X^{&#39;}X)^{-1}X^{&#39;}y\\) \\(\\frac{\\partial Q}{\\partial \\sigma^{2}}\\): \\(Q=-\\frac{n}{2}ln(\\sigma^{2})-\\frac{1}{2\\sigma^{2}}(y-X\\beta)^{&#39;}(y-X\\beta)\\) \\(\\frac{\\partial Q}{\\partial \\sigma^{2}}=\\frac{n}{\\hat{\\sigma}^{2}}-\\frac{(y-X\\hat{\\beta})^{&#39;}(y-X\\hat{\\beta})}{\\hat{\\sigma}^{2}}\\) \\(\\hat{\\sigma}^{2}=\\frac{(y-X\\hat{\\beta})^{&#39;}(y-X\\hat{\\beta})}{n}\\) б) По свойству ML оценок: \\(E(\\hat{\\beta})=\\beta\\), \\(E(\\hat{\\sigma}^{2})=\\sigma^{2}\\) с) Чтобы найти \\(Var(\\hat{\\beta_{ML}})\\), \\(Var(\\hat{\\sigma}^{2}_{ML})\\) посчитаем вторые производные: \\(\\frac{\\partial^2 Q}{\\partial \\beta^2}=-\\frac{X^{&#39;}X}{\\sigma^{2}}\\) \\(\\frac{\\partial^2 Q}{\\partial (\\sigma^{2})^2}=\\frac{n}{2(\\sigma^{2})^2}-\\frac{(y-X\\hat{\\beta})^{&#39;}(y-X\\hat{\\beta})}{(\\sigma^{2})^3}\\) \\(\\frac{\\partial^2 Q}{\\partial \\beta \\partial \\sigma^{2}}=-\\frac{X^{&#39;}(y-X\\hat{\\beta})}{(\\sigma^{2})^{2}}=-\\frac{X^{&#39;}(y-(X^{&#39;}X)^{-1}X^{&#39;}Xy)}{(\\sigma^{2})^{2}}=0\\) тогда Var \\(\\left( \\begin{matrix} \\hat{\\beta} \\\\ \\hat{\\sigma}^2 \\\\ \\end{matrix} \\right) = \\left( \\begin{matrix} -\\frac{1}{n}\\frac{\\partial^2 Q}{\\partial \\beta^2} &amp; 0 \\\\ 0 &amp; -\\frac{1}{n}\\frac{\\partial^2 Q}{\\partial (\\sigma^2)^2}\\\\ \\end{matrix} \\right)^{-1} = \\left( \\begin{matrix} \\frac{n\\sigma^2}{X&#39;X} &amp; 0 \\\\ 0 &amp; 2(\\sigma^2)^2\\\\ \\end{matrix} \\right)\\) 5.0.6 ДЗ В упражнении 1 сделать те же пункты для модели А Даны \\(A_{r\\times s}\\), \\(B_{s\\times r}\\) Записать через эти матрицы ( и их преобразования) сумму: \\(\\sum_{i=1, j=1}^n a_{ij}b_{ij}=?\\) 5.0.7 Ссылки В лекции 5 Бостонского университета подробнее изложено про матрицу-шляпницу и её свойста Да и в целом курс Бостонского университета хорош :) "],
["hints-and-hats.html", "6 Статистические свойства RSS 6.1 Упражнение 1 6.2 Теорема 6.3 Домашка", " 6 Статистические свойства RSS Датa 10/10/16 Конспект: Мария Такташева, Алексей Панков Главный спонсор этого семинара – Алеся Бердникович, которая все решила дома. объявление публикуется на правах рекламы 6.1 Упражнение 1 Найти матожидание и дисперсию оценки параметра \\(\\hat{\\beta}\\) методом максимального правдоподобия. Для этого построим функцию правдоподобия: \\[ Q = \\ln p(y) = - \\dfrac{n}{2} \\ln 2 \\pi - \\dfrac{1}{2} \\ln \\det\\left(\\sigma^2 I\\right) - \\dfrac{1}{2} (y - X\\beta)&#39; (\\sigma^2 I)^{-1} (y - X \\beta) \\rightarrow \\max_{\\hat{\\beta}, \\hat{\\sigma^2}} \\] 6.1.1 Матожидание оценки Продифференцируем функцию правдоподобия по \\(\\hat{\\beta}\\), чтобы найти оценку ML параметра \\(\\beta\\) \\[\\begin{align} \\dfrac{\\partial Q}{\\partial \\hat{\\beta}} = \\dfrac{\\partial}{\\partial \\hat{\\beta}} \\left[ -\\dfrac{1}{2} \\left(y - X\\hat{\\beta} \\right)&#39;\\left(\\sigma^2 I\\right)^{-1}\\left(y- X\\hat{\\beta} \\right) \\right] = -\\dfrac{1}{2\\sigma^2 I} \\dfrac{\\partial}{\\partial \\hat{\\beta}} \\left[\\left(y&#39; - \\hat{\\beta}&#39;X&#39;\\right)\\left(y - X\\hat{\\beta}\\right)\\right] = \\\\ = -\\dfrac{1}{2\\sigma^2 I} \\dfrac{\\partial}{\\partial \\hat{\\beta}} \\left[ y&#39;y -\\hat{\\beta}&#39;X&#39;y - y&#39;X\\hat{\\beta} - \\hat{\\beta}X&#39;X\\hat{\\beta} \\right] = 0 \\end{align}\\] А теперь заметим, что это выражение состоит из скаляров. И действительно: \\(y&#39;y = y&#39;_{1 \\times n} \\times y_{n \\times 1} = y&#39;y_{1 \\times 1} \\) \\(\\hat{\\beta}&#39;X&#39;y = \\hat{\\beta}&#39;_{1\\times k} \\times X&#39;_{k \\times n} \\times y_{n \\times 1} = \\hat{\\beta}&#39;X&#39;y_{1 \\times 1} \\) \\(y&#39;X\\hat{\\beta} = y&#39;_{1 \\times n} \\times X_{n \\times k} \\times \\hat{\\beta}_{k \\times 1} = y&#39;X\\hat{\\beta}_{1 \\times 1} \\) ну и без лишних подробностей \\(\\hat{\\beta}&#39;X&#39;X\\hat{\\beta}_{1 \\times 1} \\) При этом известно, что если матрица \\(A = A_{1 \\times 1}\\) — скаляр, то \\(A&#39;= A\\). Значит \\[ \\hat{\\beta}&#39;X&#39;y = \\left( y&#39;X\\hat{\\beta} \\right)&#39; = y&#39;X\\hat{\\beta} \\] и выражение выше принимает вид \\[ -\\dfrac{1}{2\\sigma^2 I} \\dfrac{\\partial}{\\partial \\hat{\\beta}} \\left[ y&#39;y -\\hat{\\beta}&#39;X&#39;y - 2\\hat{\\beta}X&#39;X\\hat{\\beta} \\right] = 0 \\] Вспомнив некоторые правила матричного дифференцирования, можно прийти к виду: \\[\\begin{align} -\\dfrac{1}{2\\sigma^2 I} \\left[ -2X&#39;y + X&#39;X\\hat{\\beta} + \\left(X&#39;X\\right)&#39;\\hat{\\beta} \\right] = \\nonumber \\\\ = -\\dfrac{1}{2\\sigma^2 I} \\left[ -2X&#39;y + 2X&#39;X \\hat{\\beta} \\right] = 0 \\end{align}\\] Для тех, кто не помнит, как работать с матрицами \\[ \\dfrac{\\partial}{\\partial x} x&#39;Ax = \\left(A&#39; + A \\right) \\] \\[ \\dfrac{\\partial}{\\partial x} x&#39;A = A \\] \\[ \\dfrac{\\partial}{\\partial x} Ax = A&#39; \\] В итоге мы получаем оценку \\(\\hat{\\beta}_{ML} = \\left(X&#39;X\\right)^{-1}X&#39;y\\), которая совпадает с оценкой \\(\\hat{\\beta}_{OLS}\\), построенной методом наименьших квадратов. 6.1.2 Дисперсия оценки До того как мы будем искать производную функции правдоподобия, неплохо бы заметить, что из-за свойств определителя диагональной матрицы \\[ \\det\\left(\\sigma^2 I\\right) = \\left(\\sigma^2\\right)^n \\] Зная этот хинт, можно дифференцировать функцию правдоподобия по \\(\\hat{\\sigma^2}\\): \\[\\begin{align} \\dfrac{\\partial Q}{\\partial \\hat{\\sigma^2}} = \\dfrac{1}{2} \\left(y -X\\hat{\\beta}\\right)&#39;\\left(y-X\\hat{\\beta}\\right)\\cdot \\dfrac{1}{(\\hat{\\sigma^2})^2} - \\dfrac{1}{2} \\cdot \\dfrac{n}{\\hat{\\sigma^2}} = 0 \\end{align}\\] \\[ \\left(y -X\\hat{\\beta}\\right)&#39;\\left(y-X\\hat{\\beta}\\right) = n\\hat{\\sigma^2} \\] \\[ \\hat{\\sigma^2} = \\dfrac{(y -X\\hat{\\beta})&#39;(y-X\\hat{\\beta})}{n} \\] Теперь вспоминаем, что \\(\\hat{y} = X\\hat{\\beta} \\), \\(\\hat{\\varepsilon} = y - \\hat{y} \\), а \\(\\hat{\\varepsilon}&#39;\\hat{\\varepsilon} = RSS\\), откуда: \\[ \\hat{\\sigma^2} = \\dfrac{RSS}{n} \\] 6.1.3 Тривиальщина, которая здорово упрощает жизнь Пусть у нас есть матрица \\(a_{1\\times 1}\\), тогда магически \\[ a&#39; = a \\quad \\det(a) = a \\quad tr(a) = a \\] Теперь в более общем виде c \\(Z_{n\\times m}\\). А правда ли, что \\[ \\mathbb{E} \\left(Z&#39;\\right) = \\left( \\mathbb{E} \\left( Z \\right)\\right)&#39; \\]? Да! Математическое ожидание матрицы — это матрица, в которой от каждого элемента взято математическое ожидание. Транспонирование просто переставляет элементы матрицы, не изменяя их. А это тоже верно? \\[ \\det \\left(\\mathbb{E} \\left(Z\\right)\\right) = \\mathbb{E}\\left(\\det\\left(Z\\right)\\right) \\]? Нет. Приведем простой контрпример: пусть \\(A\\) — матрица \\(2\\times 2\\), тогда её определитель легко посчитать по формуле \\[ \\det\\left(\\mathbb{E}(A)\\right) = \\det \\left( \\begin{matrix} \\mathbb{E}(a_{1,1}) &amp; \\mathbb{E}(a_{1,2}) \\\\ \\mathbb{E}(a_{2,1}) &amp; \\mathbb{E}(a_{2,2}) \\end{matrix} \\right) = \\mathbb{E}(a_{1,1})\\mathbb{E}(a_{2,2}) + ... \\ne \\mathbb{E}\\left(a_{1,1}a_{2,2} + ...\\right) = \\mathbb{E}(\\det(A)) \\] Математическое ожидание произведения не равно произведению математических ожиданий в общем случае. Однако, это может быть верно, когда элементы матрицы не зависят друг от друга. Может быть \\[ tr\\left(\\mathbb{E}\\left(Z\\right)\\right) = \\mathbb{E}\\left(tr\\left(Z\\right)\\right) \\]? Точно! Ведь след — это сумма диагональных элементов Вывод: математическое ожидание любит след и транспонирование Продолжаем. \\[ RSS = (y - X\\hat{\\beta})&#39; (y - X\\hat{\\beta}) = (y - X(X&#39;X)^{-1} X&#39; y)&#39;(...), \\] где \\(H = X(X&#39;X)^{-1} X&#39;\\) — “матрица-шляпница” или “hat matrix” 6.1.4 Магические свойства “матрицы-шляпницы” А матрица \\(X\\) у Себера называется «матрица плана» Это матрица проекции. \\[ Hy = \\hat{y}, \\quad H\\times \\text{любой вектор = проекция этого вектора на &quot;лапу&quot;} \\] Напоминаем, что “лапа” — линейная оболочка вектора \\(X\\). Два раза проецировать можно, но результат не изменится :) \\[ H^2 y = Hy = \\hat{y} \\] \\[ H&#39; = H \\] Возвращаемся к дисперсии оценки \\(\\hat{\\beta_{ML}}\\) \\[ RSS = ((I-H)y)&#39;((I-H)y) = y&#39;(I-H)&#39;(I-H)y = \\] \\((A-B)&#39; = A&#39; - B&#39; \\Rightarrow \\) \\[ = y&#39; (II - HI - IH +H^2) y = y&#39;(I-H) y \\] Поскольку \\(RSS\\) имеет размерность \\(1\\times 1\\), можно перейти к следу, для того, чтобы переставить местами множители \\[ \\mathbb{E}(RSS) = \\mathbb{E}(tr(RSS)) =\\mathbb{E}(tr(y&#39;(I-H) y) = \\] \\(tr(A\\cdot B) = tr(B \\cdot A), \\quad tr(A+B) = tr(A) + tr(B) \\Rightarrow \\) \\[ = \\mathbb{E}(tr((I-H)y&#39;y) = \\mathbb{E}(tr(yy&#39;) - tr(Hyy&#39;)) = tr(\\mathbb{E}(yy&#39;)) - tr(\\mathbb{E}(Hyy&#39;)) = tr(\\mathbb{E}(yy&#39;)) - tr(H \\mathbb{E}(yy&#39;)) = \\] \\(\\mathbb{E}(yy&#39;)_{n\\times n} =\\) ?, можно найти из \\(Var(y) = \\mathbb{E}(yy&#39;) - \\mathbb{E}(y)\\mathbb{E}(y&#39;)\\) \\(Var(y) = \\sigma^2 I\\ \\) \\(\\mathbb{E}(y)\\mathbb{E}(y&#39;) = X\\beta \\beta&#39;X&#39; \\), т.к. \\(y = X\\beta + \\varepsilon, \\mathbb{E}(y) = \\mathbb{E}(X\\beta) + \\mathbb{E}(\\varepsilon) = X\\beta \\) \\(\\Rightarrow \\mathbb{E}(yy&#39;) = \\sigma^2 I + (X\\beta \\beta&#39; X&#39;)_{n \\times n}\\) \\[ = tr((I-H)(\\sigma^2 I + X\\beta \\beta&#39; X&#39;) = tr(\\sigma^2(I-H)) = \\] \\((I-H)(X\\beta \\beta&#39;X&#39;) = X\\beta \\beta&#39;X&#39; - X\\beta \\beta&#39;X&#39; = 0\\) Хорошее свойство следа: \\(tr(X) = \\sum_i \\lambda_i \\) — равен сумме собственных чисел матрицы \\[ = \\sigma^2 (\\lambda_1 + ... + \\lambda_n) = \\] 6.1.5 Собственные числа Для матрицы \\(I\\) — \\(\\underbrace{1 ... 1}_{\\text{n штук}}\\). Для матрицы \\(H\\) — \\(\\underbrace{1 ... 1}_{\\text{k штук}}\\underbrace{0 ... 0}_{\\text{n-k штук}}\\). Множество собственных чисел устроено так, поскольку \\[ (I-H) v = \\begin{cases} 1 \\cdot v \\text{ для перпендикуляров лапе}\\\\ 0 \\cdot v \\text{ для лежащих в лапе} \\end{cases} \\] \\[ = (n-k)\\sigma^2 = E(RSS) \\] Наконец-то мы посчитали \\(E(RSS)\\). Теперь ясно, что оценка дисперсии случайной ошибки \\(\\varepsilon_i\\) методом максимального правдоподобия — смещенная! \\[ \\mathbb{E} (\\hat{\\sigma}^2_{ML}) = \\dfrac{n-k}{n} \\sigma^2 \\ne \\sigma^2 \\] Зато можно построить несмещенную \\[ \\hat{\\sigma}^2_{\\text{скорр}} = \\dfrac{RSS}{n-k} \\] 6.2 Теорема Если \\(M\\) — проектор — выполняет проецирование (\\(M&#39; = M\\), \\(M^2 = M\\)) и вектор \\(u \\sim \\mathbb{N}(0, I)\\), то \\(u&#39;Mu \\sim \\chi^2_{rk (M)}\\). Для проектора \\(rk(M)= tr(M)\\) — количество линейно независимых собственных векторов (или просто столбцов в матрице \\(M\\)) 6.3 Домашка Найти как распределено \\(\\dfrac{RSS}{\\sigma^2}\\) Посчитать руками \\(H\\), \\(I-H\\), \\(rk(H)\\), \\(rk(I-H)\\), \\(tr(H)\\), \\(tr(I-H)\\), найти закон распределения \\(\\dfrac{\\varepsilon&#39;H\\varepsilon}{\\sigma^2}\\), \\(\\dfrac{\\varepsilon&#39;(I-H)\\varepsilon}{\\sigma^2}\\), \\(\\dfrac{y&#39;(I-H)y}{\\sigma^2}\\) и \\(\\dfrac{RSS}{\\sigma^2}\\) если известно, что \\[ X = \\left( \\begin{matrix} 1 &amp; 0.1 \\\\ 1 &amp; 0.2 \\\\ 1 &amp; 0.3 \\end{matrix} \\right), \\quad y = X\\beta + \\varepsilon, \\quad \\varepsilon \\sim \\mathbb{N}(0, \\sigma^2 I) \\] "],
["holyday2-preparation.html", "7 Подготовка к празднику №2 7.1 Упражнение 1 7.2 Упражнение 2 7.3 Подготовка к контрольной! 7.4 Упражнение 3 7.5 ДЗ", " 7 Подготовка к празднику №2 Дата: 31.11.2016 Авторы: Герман Никита, Ишмаева Бэлла 7.1 Упражнение 1 Найти как распределена случайная величина: \\[ \\frac{RSS}{\\sigma^2} \\sim {?} \\] Для этого надо вспомнить теорему: Если одновременно выполнено: \\(H\\) — проектор: \\(H^2 = H\\), \\(H^T = H\\) \\(u\\sim N(0; I)\\) То: \\(u^{T}\\cdot H \\cdot u \\sim \\chi^2_{k}\\), где \\(k\\) — это размерность пространства, куда \\(H\\) проецирует. Вспомним, как распределена случайная ошибка в модели регрессии: \\[ \\varepsilon \\sim N(0; \\sigma^2{I}) \\] При этом: \\[ \\frac{RSS}{\\sigma^2} = \\frac{\\hat{\\varepsilon}^{T}\\cdot\\hat{\\varepsilon}}{\\sigma^2} \\] Теперь вспомним, что такое наблюдаемая ошибка: \\[ \\hat{\\varepsilon} = y - \\hat{y} = y - Hy = (I - H)y = (I - H)(X\\beta + \\varepsilon) = X\\beta - HX\\beta + (I - H)\\varepsilon \\] Вспомним, что \\(H=X(X^{T}X)^{-1}X^{T}\\) Также вспомним, что матрица H проецирует любой вектор на линейную оболочку, порожденную X. Тогда проекция X будет тоже X. То есть \\(HX=X\\). Тогда \\(\\hat{\\varepsilon} = (I - H)\\varepsilon\\). Отсюда: \\[ \\frac{{RSS}}{\\sigma^2} = \\frac{ \\varepsilon^{T}(I - H)^{T}\\cdot(I - H)\\varepsilon }{\\sigma^2} \\] Вспомним, что \\((I - H)^{T} = (I - H)\\) и \\((I - H)^2 = (I - H)\\). Тогда: \\[ \\frac{{RSS}}{\\sigma^2} = \\frac{ \\varepsilon^{T}(I - H)\\varepsilon}{\\sigma^2} = u^T(I-H)u, \\] где \\(u = \\frac{\\varepsilon}{\\sigma^2}\\), \\(u \\sim N(0;I)\\). Вспоминая теорему и, используя свойство, что \\({\\mathrm{rank}}(I - H) = tr(I - H) = n - k\\), получаем: \\[ \\frac{RSS}{\\sigma^2} \\sim \\chi^2_{n - k} \\] 7.2 Упражнение 2 Необходимо посчитать: \\[ E\\left(y^{T}\\cdot\\frac{1}{n}\\cdot{S}\\cdot{y}\\right) \\] Если известно, что y задается регрессионной моделью: \\[ \\begin{aligned} y &amp;= {X}\\beta + \\varepsilon \\\\ \\varepsilon &amp; \\sim N(0;\\sigma^2 I) \\end{aligned} \\] S — матрица строевого леса: \\[ S = \\begin{pmatrix} 1 &amp; \\cdots &amp; 1\\\\ \\vdots &amp; \\ddots &amp; \\vdots\\\\ 1 &amp; \\cdots &amp; 1 \\end{pmatrix} \\] Заметим, что \\(({y}^{T}\\cdot\\frac{1}{n}\\cdot{S}\\cdot{y})\\) имеет размерностнь 1 x 1. Возьмем след, потому что след матрицы 1 x 1 равняется самой матрице: \\[ tr\\left(E({y}^{T}\\cdot\\frac{1}{n}\\cdot{S}\\cdot{y})\\right) = E\\left(tr(y^{T}\\cdot\\frac{1}{n}\\cdot{S}\\cdot{y})\\right) = E\\left(\\frac{1}{n}S\\cdot tr(y^{T}y)\\right) = tr\\left(\\frac{1}{n}{S}\\cdot{E}(y^{T}y)\\right) \\] Вспомним, что: \\[ E(yy^{T}) = Var(y) + E(y)E(y^{T}) \\] Тогда: \\[ tr\\left(E(y^{T}\\cdot\\frac{1}{n}\\cdot{S}\\cdot{y})\\right) = \\frac{1}{n}tr\\left[ S(\\sigma^2 I + X\\beta\\beta^{T}X^{T})\\right] = \\frac{1}{n}{tr}(S\\sigma^2 I) + \\frac{1}{n} tr(SX\\beta\\beta^{T}X^{T}) \\] Заметим, что след матрицы \\(S\\sigma^2 I\\) равен \\(n\\sigma^2\\). Также заметим, что \\(SX\\beta\\beta^{T}X^{T}\\) - скаляр. Отсюда: \\[ tr\\left(E(y^{T}\\cdot\\frac{1}{n}\\cdot S\\cdot{y})\\right) = \\sigma^2 + \\frac{1}{n}SX\\beta\\beta^{T}X^{T} \\] 7.3 Подготовка к контрольной! 7.3.0.1 Сначала вспомним модель парной регрессии Общая формула нахождения оценок \\(\\beta\\) для множественной регрессии: \\[ \\hat{\\beta} = (X^{T}X)^{-1}X^{T}y \\] А для парной: \\[ \\begin{aligned} {y}_{i} &amp;= \\beta_1 + \\beta_2 x_{i} + \\varepsilon_{i}, \\, \\text{где} \\\\ X &amp;= \\begin{pmatrix} 1 &amp; x_1\\\\ \\vdots &amp; \\vdots\\\\ 1 &amp; x_n \\end{pmatrix}\\\\ X^{T}X &amp;= \\begin{pmatrix} n &amp; \\sum_{i}x_{i}\\\\ \\sum_{i}x_{i} &amp; \\sum_{i}x_{i^2} \\end{pmatrix}\\\\ X^{T}y &amp;= \\begin{pmatrix} \\sum_{i} y_{i}\\\\ \\sum_{i} x_{i}y_{i}\\\\ \\end{pmatrix} \\end{aligned} \\] В этом случае можно воспользоваться шаманским способом обращения матриц: \\[ A^{-1} = \\begin{pmatrix} a &amp; b\\\\ c &amp; d \\end{pmatrix}^{-1} =\\frac{1}{detA} \\begin{pmatrix} d &amp; -b\\\\ -c &amp; a \\end{pmatrix} \\] 7.3.0.2 А теперь перейдем к задачам про доверительный интервал Когда мы решали подобного рода задачи в прошлом году, мы обычно находили статистику и сравнивали ее с критическим значением нормального распределения, распределения Стьюдента или Фишера, в зависимости от того, как распределена была сама статистика. В нашем случае нам нужно проверять гипотезы и строить доверительные интервалы для \\(\\beta\\), а значит нужно понять, какое распределение имеет статистика: \\[ \\frac{\\hat\\beta - \\beta}{{se}\\left(\\hat\\beta\\right)} \\] Если с числителем еще более или менее понятно: \\(\\hat\\beta\\) имеет нормальное распределение, то вот со знаменателем стоит разобраться, что мы и будем делать дальше. В домашнем задании мы уже выводили, что: \\[ \\begin{cases} Cov\\left(\\hat\\varepsilon,\\hat\\beta\\right) = 0\\\\ Cov\\left(\\hat\\beta\\right) = \\sigma^2 \\left(X^{T}X\\right)^{-1} \\end{cases} \\] А скорректированная \\(ML\\) оценка для сигмы-квадрат : \\[ \\hat\\sigma^2 = \\left(\\frac{RSS}{n-k}\\right) \\] Стоит отметить, что в начале семинара мы уже показали, что, если отмасштабировать, \\(RSS\\) будет иметь распределение \\(\\chi^2_{n - k}\\), которое здесь делится на количество степеней свободы \\(n - k\\,\\)(должно напоминать нам \\(t-\\) или \\(F-\\) распределение) Теперь вернемся к условию \\(Cov(\\hat\\varepsilon,\\hat\\beta) = 0\\): вообще говоря, равенство нулю ковариации не говорит нам о независимости случайных величин, но в нашем случае \\(\\hat\\epsilon\\) и \\(\\hat\\beta\\) имеют совместное нормальное распределение, а значит мы можем сказать, что они независимы. Имеем: \\[ Var\\left(\\hat\\beta\\right) = \\sigma^2 \\left(X^{T}X\\right)^{-1} \\hat{Var}\\left(\\hat\\beta\\right) = \\hat{\\sigma^2} \\left(X^{T}X\\right)^{-1} \\hat{Var}\\left(\\hat\\beta\\right) = \\left(\\frac{RSS}{n-k}\\right) \\left(X^{T}X\\right)^{-1} \\] Матрица \\(\\hat{Var}\\left(\\hat\\beta\\right)\\) имеет вид: \\[ \\begin{aligned} \\begin{pmatrix} \\hat{Var}\\left(\\hat\\beta_1\\right) &amp; \\cdots &amp; \\hat{Cov}\\left(\\hat\\beta_n,\\hat\\beta_1\\right)\\\\ \\vdots &amp; \\ddots &amp; \\vdots\\\\ \\hat{Cov}\\left(\\hat\\beta_1,\\hat\\beta_n\\right) &amp; \\cdots &amp; \\hat{Var}\\left(\\hat\\beta_{n}\\right) \\end{pmatrix}\\\\ \\end{aligned} \\] А каждый элемент матрицы мы можем посчитать исходя из данных. Вспомним, как выглядит распределение Стьюдента: \\[ \\begin{aligned} \\frac{N\\left({0,1}\\right)}{\\sqrt{\\frac{\\chi^2_{r}}{r}}} \\sim t_{r} \\end{aligned} \\] Тогда мы можем получить следующую теорему: Если: \\(Y = X\\beta +\\varepsilon\\) \\(\\varepsilon \\sim N\\left(0, \\sigma^2 I\\right)\\) То: \\[ \\frac{\\hat\\beta_{j} - \\beta_{j}}{se\\left(\\hat\\beta_{j}\\right)} \\sim {t}_{n-k}, \\] где \\(se = \\sqrt{\\hat{Var}\\left(\\hat\\beta_{j}\\right)}\\) — это стандартная ошибка. 7.4 Упражнение 3 Дано: \\(\\sum_{i} x_{i} = 5\\), \\(\\sum_{i} y_{i} = 2\\), \\(\\sum_{i} x_{i}y_{i} = 20\\), \\(\\sum_{i}x_{i}^2 = 10\\), \\(\\sum_{i} y_{i}^2 = 80\\), \\(n=5\\) \\(y_{i}=\\beta{x}_{i}+\\varepsilon_{i}\\) \\(\\varepsilon_{i} \\sim N\\left(0, \\sigma^2\\right)\\) Найти: \\(X^{T}X\\), \\(X^{T}y\\) \\(\\hat\\beta\\), \\(RSS\\), \\(\\hat\\epsilon^2\\), \\(se\\left(\\hat\\beta\\right)\\) Построить 95% доверительный интервал для \\(\\hat\\beta\\) при \\(\\alpha=0.05\\) и проверить гипотезу: \\[ \\begin{aligned} H_{0}:\\, \\beta &amp;= 0\\\\ H_{a}:\\, \\beta &amp;\\neq 0 \\end{aligned} \\] Решение: a) \\(X^{T}X = \\sum_{i}x_{i}^2 = 10\\), \\(X^{T}y = \\sum_{i}x_{i}y_{i} = 20\\) b) \\(\\hat\\beta = (X^{T}X)^{-1}{X}^{{T}}{y} = \\left(10^{-1}20\\right) = 2\\). Для нахождения \\(RSS\\), нужно вспомнить, что: \\(H={X}({X}^{{T}}{X})^{-1}{X}^{{T}}\\) \\[ RSS = y^{T}\\left(I-H\\right)y = y^{T}Iy - y^{T}Hy =y^{T}y - y^{T}X\\left(X^{T}X\\right)^{-1}X^{T}y = \\sum_{i} y_{i}^2 - \\sum_{i}x_{i}y_{i}\\left(\\sum_{i} x_{i}^2\\right)^{-1}\\sum_{i}x_{i}y_{i} \\] \\[ \\begin{aligned} RSS &amp;= 80 - 20\\frac{1}{10}20 = 40\\\\ \\hat\\sigma^2 &amp;= \\frac{RSS}{n-k} = \\frac{40}{4} = 10\\\\ \\hat{Var}\\left(\\hat\\beta\\right) &amp;= \\hat\\sigma^2\\left(X^{T}X\\right)^{-1} = 10\\frac{1}{10} = 1\\\\ se\\left(\\hat\\beta\\right) &amp;= \\sqrt{\\hat{Var}\\left(\\hat\\beta\\right)} = 1 \\end{aligned} \\] c) В общем случае доверительный интервал выглядит так: \\[ \\beta\\in\\left[\\hat\\beta - t_{crit}\\cdot{se}\\left(\\hat\\beta\\right);\\hat\\beta + t_{crit}\\cdot{se}\\left(\\hat\\beta\\right)\\right] \\] Находим в таблице критическое значение для t-распределения с \\(\\left({n-1}\\right) = 4\\) степенями свободы и 5% уровнем значимости: \\({t}_{crit} = 2,77\\). Тогда наш интервал выглядит следующим образом: \\[ \\beta\\in[2 - 2,77;2 + 2,77] \\] Чтобы проверить гипотезу, найдем \\({t}\\) наблюдаемое: \\[ \\frac{\\hat\\beta - \\beta}{se\\left(\\hat\\beta\\right)} = \\frac{2-0}{1} = 2 \\] Получаем, что \\({H}_{{0}}\\) не отвергается. 7.5 ДЗ Демоверсия и разделы 3-4 "]
]
