[
["index.html", "Конспект семинаров по метрике-2016 О проекте", " Конспект семинаров по метрике-2016 Студенты ИП и Борис Демешев 2016-10-06 О проекте Метрика с R :) library(&quot;knitr&quot;) library(&quot;tikzDevice&quot;) activateTikz &lt;- function() { # tikz plots options options(tikzDefaultEngine = &quot;xetex&quot;) # cash font metrics for speed: # options(tikzMetricsDictionary = &quot;./tikz_metrics&quot;) add_xelatex &lt;- c(&quot;\\\\defaultfontfeatures{Ligatures=TeX, Scale=MatchLowercase}&quot;, &quot;\\\\setmainfont{Linux Libertine O}&quot;, &quot;\\\\setmonofont{Linux Libertine O}&quot;, &quot;\\\\setsansfont{Linux Libertine O}&quot;, &quot;\\\\newfontfamily{\\\\cyrillicfonttt}{Linux Libertine O}&quot;, &quot;\\\\newfontfamily{\\\\cyrillicfont}{Linux Libertine O}&quot;, &quot;\\\\newfontfamily{\\\\cyrillicfontsf}{Linux Libertine O}&quot;) options(tikzXelatexPackages = c(getOption(&quot;tikzXelatexPackages&quot;), add_xelatex)) # does remove warnings: # it is important to remove fontenc package wich is loaded by default options(tikzUnicodeMetricPackages = c(&quot;\\\\usetikzlibrary{calc}&quot;, &quot;\\\\usepackage{fontspec, xunicode}&quot;, add_xelatex)) opts_chunk$set(dev = &quot;tikz&quot;, dev.args = list(pointsize = 11)) } colFmt &lt;- function(x, color) { outputFormat &lt;- opts_knit$get(&quot;rmarkdown.pandoc.to&quot;) if (outputFormat == &quot;latex&quot;) { result &lt;- paste0(&quot;\\\\textcolor{&quot;, color, &quot;}{&quot;, x, &quot;}&quot;) } else if (outputFormat %in% c(&quot;html&quot;, &quot;epub&quot;)) { result &lt;- paste0(&quot;&lt;font color=&#39;&quot;, color, &quot;&#39;&gt;&quot;, x, &quot;&lt;/font&gt;&quot;) } else { result &lt;- x } return(result) } outputFormat &lt;- opts_knit$get(&quot;rmarkdown.pandoc.to&quot;) if (outputFormat == &quot;latex&quot;) { activateTikz() # другую тему для ggplot2 выставить? } Данная версия конспекта скомпилирована для html. library(&quot;ggplot2&quot;) # графики library(&quot;sandwich&quot;) # оценка Var для гетероскедастичности library(&quot;lmtest&quot;) # тест Бройша-Пагана library(&quot;dplyr&quot;) # манипуляции с данными library(&quot;broom&quot;) # преобразование всего и вся в стандартные таблички library(&quot;data.table&quot;) # манипуляции с данными library(&quot;reshape2&quot;) # преобразование длинных таблиц в широкие library(&quot;tidyr&quot;) # причесывание данных "],
["ols-intro.html", "1 Метод наименьших квадратов 1.1 Основная задача 1.2 Реализация в R: 1.3 Домашнее задание", " 1 Метод наименьших квадратов Конспект: Бердникович Алеся, Головина Мария дата: 05.09.2016 1.1 Основная задача Маша каждый день ловит покемонов и решает задачи по теории вероятностей. Пусть x и y - случайные величины, \\(x_i\\) - количество решённых в i-тый день задач, а \\(y_i\\) - количество пойманных в i-тый день покемонов. Результаты наблюдения за действиями Маши представлены в таблице: День \\(x_i\\) \\(y_i\\) 1 1 10 2 2 0 3 0 4 Необходимо определить, как количество пойманных за день покемонов зависит от количества решённых за день задач. Предположим, что регрессионная модель имеет линейный вид \\(y_i = \\beta_1+\\beta_2 x_i + \\epsilon_i\\), где коэффициенты \\(\\beta_1, \\beta_2\\) неизвестны и должны быть оценены, а \\(\\epsilon_i\\) - случайная величина. Тогда прогнозируемая зависимость имеет вид \\(\\hat{y}_i=\\hat{\\beta}_1+\\hat{\\beta}_2x_i\\). 1.1.1 Метод наименьших квадратов (OLS): \\(y_i-\\hat{y}_i\\) - ошибка прогноза, которую нужно минимизировать. Штрафная функция: \\[ Q(\\hat{\\beta}_1,\\hat{\\beta}_2) = (y_1-\\hat{y}_1)^2 + (y_2-\\hat{y}_2)^2 + (y_3-\\hat{y}_3)^2 = (y_1-(\\hat{\\beta}_1+\\hat{\\beta}_2 x_1))^2 + (y_2-(\\hat{\\beta}_1+\\hat{\\beta}_2x_2))^2 + (y_3-(\\hat{\\beta}_1+\\hat{\\beta}_2x_3))^2 \\to min_{\\hat{\\beta}_1,\\hat{\\beta}_2} \\] 1.1.2 Метод наименьших модулей (LAD): Альтернативный метод минимизации ошибок прогноза. Отличие заключается в виде штрафной функции: \\[Q(\\hat{\\beta}_1,\\hat{\\beta}_2) = |y_1-\\hat{y_1}|^2 + |y_2-\\hat{y}_2|^2 + |y_3-\\hat{y}_3|^2 \\to min_{\\hat{\\beta}_1,\\hat{\\beta}_2}\\] Найдём \\(\\hat{\\beta}_1,\\hat{\\beta}_2\\) в нашей задаче методом наименьших квадратов: \\[Q(\\hat{\\beta}_1,\\hat{\\beta}_2) = (y_1-(\\hat{\\beta}_1+\\hat{\\beta}_2x_1)^2 + (y_2-(\\hat{\\beta}_1+\\hat{\\beta}_2x_2))^2 + (y_3-(\\hat{\\beta}_1+\\hat{\\beta}_2x_3))^2\\] \\[Q(\\hat{\\beta}_1,\\hat{\\beta}_2) = (10-(\\hat{\\beta}_1+\\hat{\\beta}_2)^2 + (0-(\\hat{\\beta}_1+2\\hat{\\beta}_2))^2 + (4-(\\hat{\\beta}_1))^2 \\to min\\] \\[\\frac{\\partial Q}{\\partial \\hat{\\beta}_1} = -28 + 6\\hat{\\beta}_1 + 6\\hat{\\beta}_2 = 0\\] \\[\\frac{\\partial Q}{\\partial \\hat{\\beta}_2} = -20 + 6\\hat{\\beta}_1 + 10\\hat{\\beta}_2 = 0\\] \\[\\hat{\\beta}_1 = \\frac{20}{3}, \\hat{\\beta}_2 = -2\\] Искомая оценка зависимости числа пойманных покемонов от числа решённых задач: \\[\\hat{y}_i = \\frac{20}{3} - 2x_i\\] 1.2 Реализация в R: x &lt;- c(1, 2, 0) y &lt;- c(10, 0, 4) md &lt;- data.frame(problem = x, pokemon = y) md ## problem pokemon ## 1 1 10 ## 2 2 0 ## 3 0 4 Восстановление линейной зависимости методом наименьших квадратов: model_1_ols &lt;- lm(data = md, pokemon~problem) summary(model_1_ols) ## ## Call: ## lm(formula = pokemon ~ problem, data = md) ## ## Residuals: ## 1 2 3 ## 5.333 -2.667 -2.667 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.667 5.963 1.118 0.465 ## problem -2.000 4.619 -0.433 0.740 ## ## Residual standard error: 6.532 on 1 degrees of freedom ## Multiple R-squared: 0.1579, Adjusted R-squared: -0.6842 ## F-statistic: 0.1875 on 1 and 1 DF, p-value: 0.7399 Подключаем нужный пакет: library(&quot;quantreg&quot;) Если пакета не установлен, то это исправляется командой install.packages(&quot;quantreg&quot;) Восстановление линейной зависимости методом наименьших модулей: model_1_lad &lt;- rq(data = md, pokemon~problem) summary(model_1_lad) ## ## Call: rq(formula = pokemon ~ problem, data = md) ## ## tau: [1] 0.5 ## ## Coefficients: ## coefficients lower bd upper bd ## (Intercept) 4.000000e+00 -1.797693e+308 1.797693e+308 ## problem -2.000000e+00 -1.797693e+308 1.797693e+308 Предположим теперь иную модель зависимости \\(y_i = \\hat{\\beta}x_i\\), ищем оценку единственного неизвестного коэффициента \\(\\hat{\\beta}\\) с помощью метода наименьших модулей. Штрафная функция примет вид \\[Q(\\hat{\\beta}) = |10-\\hat{\\beta}| + |0-\\hat{\\beta}| + |4-0| \\to min\\] Точки изломов функции находятся в нулях подмодульных выражений: \\(\\hat{\\beta}=0\\) и \\(\\hat{\\beta}=10\\). Функция принимает наименьшее значение при \\(\\hat{\\beta}=0\\) (см. график), что говорит об отсутствии зависимости числа пойманных покемонов от числа решённых задач. 1.2.1 График штрафной функции: x &lt;- seq(-10, 20, 0.001) fx &lt;- (x &lt;= 0) * (14 - 3 * x) + (x &gt; 0 &amp; x &lt; 10) * (14 + x) + (x &gt;= 10) * (3*x - 6) plot(x = x, y = fx, xlab = expression(hat(beta)), ylab = &#39;Q&#39;, pch = 20, col = &#39;orchid4&#39;) 1.3 Домашнее задание Вывести общие формулы для коэффициентов \\(\\hat{\\beta}, \\hat{\\beta}_1, \\hat{\\beta}_2\\), используя МНК-оценку, при условии, что: \\(y_i = \\beta x_i + \\epsilon_i, \\ \\hat{y}_i = \\hat{\\beta} x_i\\); \\(y_i = \\hat{\\beta}_1 + \\hat{\\beta}_2 x_i + \\epsilon_i, \\ \\hat{y}_i = \\hat{\\beta}_1+\\hat{\\beta}_2 x_i + \\epsilon_i\\). "],
["recall-all.html", "2 Вспомнить всё", " 2 Вспомнить всё "],
["ols-geometry.html", "3 Геометрия МНК 3.1 Обозначения 3.2 Ныряем в \\(n\\)-мерное пространство 3.3 Больше проекций", " 3 Геометрия МНК конспект: Света Колесниченко дата: 19 сентября 2016 3.1 Обозначения Варианты представления регрессии: Скалярный вариант: \\(\\hat y_{i} = \\hat \\beta_1 + \\hat \\beta_2\\, x_{i} + \\hat \\beta_3\\, z_{i}\\) Векторный вариант: \\(\\hat y = \\hat \\beta_1\\, e + \\hat \\beta_2\\, x + \\hat \\beta_3\\, z\\) \\[ \\begin{matrix} &amp; e = \\vec 1 = \\\\ \\end{matrix} \\begin{pmatrix} 1 \\\\ \\vdots \\\\ 1 \\end{pmatrix} \\begin{matrix} \\textit{ - единичный вектор размерности $n \\times 1$}, \\end{matrix} \\] \\[ \\begin{matrix} &amp; x = \\\\ \\end{matrix} \\begin{pmatrix} x_{1} \\\\ \\vdots \\\\ x_{n} \\end{pmatrix} \\begin{matrix} &amp; y = \\\\ \\end{matrix} \\begin{pmatrix} y_{1} \\\\ \\vdots \\\\ y_{n} \\end{pmatrix} \\begin{matrix} &amp; z = \\\\ \\end{matrix} \\begin{pmatrix} z_{1} \\\\ \\vdots \\\\ z_{n} \\end{pmatrix} \\textit{ - векторы переменных} \\] Количество наблюдений = \\(n\\), количество коэффициентов \\(\\beta\\) = количество регрессоров = \\(k\\). Матричный вариант: \\(\\hat y = X\\, \\hat \\beta\\) \\[ \\begin{matrix} &amp; \\hat \\beta = \\\\ \\end{matrix} \\begin{pmatrix} \\hat \\beta_{1} \\\\ \\vdots \\\\ \\hat \\beta_{k} \\end{pmatrix} \\begin{matrix} \\textit{ - вектор размера } k\\times 1, \\end{matrix} \\] \\[ \\begin{matrix} &amp; X = \\\\ \\end{matrix} \\begin{pmatrix} 1 &amp; x_{1} &amp; z_{1} \\\\ \\vdots &amp; \\vdots &amp; \\vdots \\\\ 1 &amp; x_n &amp; z_{n} \\end{pmatrix} \\begin{matrix} \\textit{ - матрица размера } n \\times k \\end{matrix} \\] Конвенция об обозначениях: \\(y, \\beta, \\hat \\beta, x, z\\) - векторы \\(y_{i}, \\beta_{j}, \\hat \\beta_{7}, x_{45}, z_{37}\\) - числа (скаляры) \\(\\Omega, X, H\\) - матрицы 3.2 Ныряем в \\(n\\)-мерное пространство \\[ \\min_{i\\in I} \\sum_{i=1}^n (y_{i} - \\hat y_{i})^{2} = \\min_{i\\in I} \\sum_{i=1}^n |y_{i} - \\hat y_{i}|^{2} \\textit{ - минимизируем квадрат длины вектора} \\] \\[ \\begin{pmatrix} \\bar y \\\\ \\vdots \\\\ \\bar y \\end{pmatrix} \\begin{matrix} &amp; = \\bar y \\cdot \\\\ \\end{matrix} \\begin{pmatrix} 1 \\\\ \\vdots \\\\ 1 \\end{pmatrix} \\begin{matrix} &amp; = \\bar y \\cdot \\vec 1 \\end{matrix} \\] \\(\\bar y = \\hat y\\) - т.к. \\(\\bar y \\cdot \\vec 1 = \\hat y \\cdot \\vec 1\\) среднее значение = среднее значение прогнозов (\\(\\hat y_{i}\\)) Картиночка Лапы “Лапа” = \\(Lin (e, x, z) \\leftarrow\\) выбираем через e, x, z положение \\(\\hat y\\) \\(\\hat y\\) - проекция y на “лапу” y - линейная комбинация e, x, z \\(\\rightarrow\\) лежит в линейной оболочке этих векторов \\(\\hat \\varepsilon = y - \\hat y\\) - вектор “остатков”/ошибок прогнозов/resideals \\(\\hat \\varepsilon \\,\\bot\\, e, \\hat \\varepsilon \\,\\bot\\, x, \\hat \\varepsilon \\,\\bot\\, y\\) \\(\\hat \\varepsilon \\cdot \\vec 1 = 0\\), \\(\\hat \\varepsilon \\cdot x = 0\\), \\(\\hat \\varepsilon \\cdot z = 0\\) \\(\\leftarrow\\) скалярное произведение векторов (ссыль подробнее) перпендикулярных векторов равно 0. \\(\\hat \\varepsilon \\, \\bot \\, \\textit{Лапа} \\rightarrow \\hat \\varepsilon \\, \\bot \\, \\textit{любому вектору, лежащему в Лапе}\\) Великая Теорема о 3 перпендикулярах и аж в 2 формулировках и с чертёжиком. \\[ \\sum_{i=1}^n \\hat \\varepsilon_{i}=0 , \\sum_{i=1}^n \\hat \\varepsilon_{i} x_{i}=0 \\] \\[ \\begin{matrix} &amp; X&#39; = \\\\ \\end{matrix} \\begin{pmatrix} 1 &amp; \\dots &amp; 1\\\\ x &amp; \\dots &amp; x_{n} \\\\ z &amp; \\dots &amp; z_{n} \\end{pmatrix} \\begin{matrix} \\textit{ - единичный вектор размерности $n \\times 1$} \\end{matrix} \\begin{matrix} &amp; \\hat \\varepsilon = \\\\ \\end{matrix} \\begin{pmatrix} \\hat \\varepsilon_{1} \\\\ \\vdots \\\\ \\hat \\varepsilon_{n} \\end{pmatrix} \\begin{matrix} \\textit{ - единичный вектор размерности $n \\times 1$} \\end{matrix} \\] Условие ортогональности: \\(X&#39; \\cdot \\hat \\varepsilon = 0\\) - размерность этого нуля - \\(k \\times 1\\) \\(\\hat y = X \\cdot \\hat \\beta \\rightarrow \\hat \\beta = \\frac{\\hat y}{X} = \\frac{\\hat \\varepsilon - Y}{X}\\) 3.2.1 Упражнение 1 Выведите \\(\\hat \\beta\\) из \\(X&#39;\\cdot(y-X\\cdot\\hat \\beta) = 0\\) \\(\\hat \\varepsilon = y - \\hat y = y - X\\cdot\\hat \\beta\\) \\(X\\) - задает “лапу”. \\(\\hat \\beta\\) - отвечает за то, с каким весом в \\(\\hat y\\) входят базисные векторы «лапы». \\(\\hat \\beta = (\\sum_{i=1}^n x^2_{i})^{-1}\\, \\sum_{i=1}^n x_{i} y_{i}=0\\) - для \\(\\hat y_{i} = \\hat \\beta \\, x_{i}\\) \\(X&#39;\\cdot y = X&#39;\\cdot X\\cdot \\hat \\beta\\) \\((X&#39; \\cdot X)^{-1}\\cdot X&#39; \\cdot y = (X&#39; \\cdot X)^{-1}\\cdot X&#39;\\cdot X \\cdot \\hat \\beta\\) \\(\\hat \\beta = (X&#39; \\cdot X)^{-1}\\cdot X&#39;\\cdot y\\) 3.3 Больше проекций 3.3.1 Упражнение 2 Спроецируйте вектор \\(\\,\\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\end{pmatrix}\\,\\) на прямую, порождённую вектором \\(\\,\\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{pmatrix}\\,.\\) Визуализация задачи \\[ \\begin{matrix} &amp; \\hat \\beta = \\\\ \\end{matrix} \\begin{pmatrix} 1 &amp; 1 &amp; 1 &amp; 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{pmatrix} \\begin{pmatrix} 1 &amp; 1 &amp; 1 &amp; 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\end{pmatrix} \\begin{matrix} = 4^{-1} \\cdot 10 \\cdot \\frac{1}{4} \\cdot 10 = 2.5 \\\\ \\end{matrix} \\] \\[ \\begin{matrix} &amp; \\hat y = X \\cdot \\hat \\beta = \\\\ \\end{matrix} \\begin{pmatrix} 2.5 \\\\ 2.5 \\\\ 2.5 \\\\ 2.5 \\end{pmatrix} \\begin{matrix} = \\\\ \\end{matrix} \\begin{pmatrix} \\bar y \\\\ \\bar y \\\\ \\bar y \\\\ \\bar y \\end{pmatrix} \\] \\[ \\bar y = \\frac{1+2+3+4}{4} = 2.5 \\] Проекция вектора на прямую из единиц даёт вектор из средних. By the way, крутые читщиты по матрицам и основам линейной алгебры. 3.3.2 Упражнение 3 Сформулируйте все теоремы Пифагора \\(\\{\\hat \\varepsilon, \\hat y - \\bar y \\cdot \\vec 1, y - \\bar y \\cdot \\vec 1\\}\\,\\) Чертёжик одного из треугольников По Теореме Пифагора: \\[ |y - \\bar y \\cdot \\vec 1|^{2} = |\\hat \\varepsilon|^2 + |\\hat y - \\bar y \\cdot \\vec 1|^2 \\] \\[ \\sum_{i=1}^n (y_{i}- \\bar y)^2 = \\sum_{i=1}^n \\hat \\varepsilon^2_{i} + \\sum_{i=1}^n (\\hat y_{i}- \\bar y)^2 \\] \\[ \\begin{matrix} &amp; y_{i}- \\bar y = \\\\ \\end{matrix} \\begin{pmatrix} y_{1} - \\bar y\\\\ \\vdots \\\\ y_{n} - \\bar y \\end{pmatrix} \\] Полное задание см. в Задачнике по координатам: 4.23, 4.24, 4.25 Коэффициент детерминации (\\(R^2\\)) - примитивный показатель качества прогнозов. \\[ R^2 = \\frac{ESS}{TSS} = \\frac{\\sum_{i=1}^n \\hat \\varepsilon^2_{i}}{\\sum_{i=1}^n (y_{i}- \\bar y)^2} = \\frac{\\sum_{i=1}^n (y_{i}- \\hat y)^2}{\\sum_{i=1}^n (y_{i}- \\bar y)^2} = \\frac{\\text{residial sum of squares (cумма квадратов остатков)}}{\\text{total sum of squares (полная сумма квадратов)}} \\] \\[ ESS = \\sum_{i=1}^n (\\hat y_{i}- \\bar y)^2 = \\, \\text{explained sum of squares (&quot;объясненная&quot; сумма квадратов)} \\] В МНК работает (и точно только в нём!) соотношение: \\(RSS + ESS = TSS\\). В МНК решается задача минимизации RSS. Если прогнозы \\(\\hat y_i\\) идеально совпадают с \\(y_i\\), то \\(R^2 = 1 \\Rightarrow ESS = TSS\\). \\(R^2 \\in [0;1]\\, , R^2 = \\cos^2 \\rho\\) \\(\\hat y\\) ближе к \\(y\\) с ростом «лапы» \\(\\Rightarrow\\) \\(\\angle \\rho \\downarrow \\, \\rightarrow R^2\\,\\) т.к. \\(\\cos^2 \\rho \\uparrow\\) 3.3.2.1 ДЗ: 1.1, 1.2, 1.7, 1.12, 1.13, 4.13 (1-6), 4.23, 4.24, 4.25 из Задачника 3.3.2.1.1 Полезные ссылки: Репозиторий курса метрики и теории вероятностей "],
["matrix-fight.html", "4 Борьба с матрицами", " 4 Борьба с матрицами дата: 26 сентября 2016 конспект: Вика Шрамова, Эдуард Аюнц Семинар посвящен работе с матрицами - матричному дифференцированию и представлению многомерных случайных величин при помощи матриц. Перед тем как приступить к работе с матрицами, полезно повторить основные свойства операций над матрицами: \\(A(B+C) = AB+ AC\\) \\((A+B)^T=A^T + B^T\\) \\((AB)^T = B^T A^T\\) \\((AB)^{-1} = B^{-1} A^{-1}\\) \\((A^{-1})^T = (A^T)^{-1}\\) Производные следа и определителя: \\(tr(AB)&#39;_A = B^T\\) \\(det(A)&#39;_A = det(A) (A^{-1})^T\\) \\((log det A(x) )&#39;_x = tr(A^{-1} A&#39;_x)\\) След и определитель: \\(det(AB) = det(A) det(B)\\) \\(det(A^{-1})= 1/det(A)\\) \\(det(A) = \\prod_j \\lambda_j\\) \\(tr(A) = \\sum_j A_{jj} = \\sum_j \\lambda_j\\) \\(tr(ABC) = tr(BCA) = tr(CAB)\\) Для начала напомним о разнице между одномерными и многомерными случайными величинами. Обозначим \\(y\\) как случайный вектор \\(\\left( \\begin{matrix} y_1 \\\\ \\vdots \\\\ y_n \\end{matrix} \\right)\\). Одномерную случайную величину будем обозначать маленькими латинскими буквами с индексами: \\(y_1\\). \\(Var(y)\\) = \\(\\left( \\begin{matrix} Var(y_1) &amp; Cov (y_1,_2) &amp; \\hdots Cov (y_1,_n) \\\\ \\vdots &amp; \\vdots &amp; \\vdots \\\\ Cov(y_k,y_1) &amp; Var (y_k) &amp; \\hdots Cov (y_k,y_n) \\\\ \\vdots &amp; \\vdots &amp; \\vdots \\\\ Cov(y_n,y_1) &amp; Cov (y_n,_2) &amp; \\hdots Var (y_n) \\end{matrix} \\right)\\) Из такой записи ковариции векторов очевидно, что если в формуле ковариации поменять местами векторы, то их матрица ковариации будет являться транспонированной матрицой ковариации векторов в исходной последовательности. \\(Cov(y,z) =Cov (z,y)ˆ{T}\\) Упражнение Дана матрица \\(A = \\left( \\begin{matrix} 2 &amp; 0 \\\\ 1 &amp; 3\\\\ \\end{matrix} \\right)\\) и случайный вектор \\(y = \\left( \\begin{matrix} y_1 \\\\ y_2 \\end{matrix} \\right)\\) с матожиданием \\(E(y) = \\left( \\begin{matrix} 2 \\\\ 7 \\end{matrix} \\right)\\) и дисперсией \\(Var(y) = \\left( \\begin{matrix} 3 &amp; 1 \\\\ 1 &amp; 1 \\end{matrix} \\right)\\) Требуется найти \\(E(z), Var(z), Cov(y,z).\\) Решение \\(E(z) = A \\cdot E(y) = \\left( \\begin{matrix} 2 &amp; 0 \\\\ 1 &amp; 3\\\\ \\end{matrix} \\right)\\) \\(\\cdot\\) \\(\\left( \\begin{matrix} 2 \\\\ 7 \\end{matrix} \\right)\\) = \\(\\left( \\begin{matrix} 4 \\\\ 23 \\end{matrix} \\right)\\) \\(Var(z)\\) = \\(A \\cdot Var(y) \\cdot Aˆ{T}\\) = \\(\\left( \\begin{matrix} 2 &amp; 0 \\\\ 1 &amp; 3\\\\ \\end{matrix} \\right)\\) \\(\\cdot\\) \\(\\left( \\begin{matrix} 3 &amp; 1 \\\\ 1 &amp; 7\\\\ \\end{matrix} \\right)\\) \\(\\cdot\\) \\(\\left( \\begin{matrix} 2 &amp; 1 \\\\ 0 &amp; 3\\\\ \\end{matrix} \\right)\\) = \\(\\left( \\begin{matrix} 12 &amp; 12 \\\\ 12 &amp; 72\\\\ \\end{matrix} \\right)\\) \\(t = Ay\\) = \\(\\left( \\begin{matrix} 2y_1 \\\\ y_1 +3y_2 \\end{matrix} \\right)\\) \\(Cov(y,z)\\) = \\(\\left( \\begin{matrix} Cov (y_1, z_1) &amp; Cov (y_1,z_2) \\\\ Cov(y_2,z_1) &amp; Cov (y_1, z_2) \\end{matrix} \\right)\\) = \\(Cov(y, Ay) = Cov(y,y) Aˆ{T}\\) = \\(\\left( \\begin{matrix} 3 &amp; 1 \\\\ 1 &amp; 7\\\\ \\end{matrix} \\right)\\) \\(\\cdot\\) \\(\\left( \\begin{matrix} 2 &amp; 1 \\\\ 0 &amp; 3\\\\ \\end{matrix} \\right)\\) Упражнение Предположим, существует истинная зависимость \\(y = X\\beta +\\varepsilon\\) между оцениваемыми величиными. При оценивании параметров модели МНК будут фигурировать следующие величины: \\(y, \\hat{y}, \\varepsilon, \\hat{\\varepsilon}, \\beta , \\hat{\\beta}\\). Оценим все матожидания, дисперсии и ковариации указанных величин. Перед тем, как начать, необходимо сделать оговорку, что существуют две парадигмы исследования: 1) Предполагается, что матрица X является детерминированной. 2) Матрица X состоит из случайных величин. В ходе нашего курса мы будем работать со случайными X, однако пока будем считать, что матрица Х детерминирована. Решение Оценка \\(\\hat{\\beta} = (X&#39;X)^{T} Xy\\), \\(\\hat{\\varepsilon} = y- \\hat{y}\\), \\(\\hat{y} =X\\hat{\\beta}\\) Найдем матожидания: \\(E(\\beta) = beta,\\) так как \\(\\beta\\) – вектор неизвестных констант \\(E(\\epsilon) = 0\\) (по предпосылкам МНК) \\(E(y) = E(X\\beta + \\epsilon) = XE(\\beta) + E(\\epsilon) = X\\beta\\) \\(E(\\hat{y}) = E(X\\hat{\\beta}) = XE(\\hat{\\beta}) = XE((X^{&#39;}X)^{-1}X^{&#39;}y) = X(X^{&#39;}X)^{-1}X^{&#39;}E(y) = X(X^{&#39;}X)^{-1}X^{&#39;}X\\beta = X\\beta\\) \\(E(\\hat{\\epsilon}) = E(y - \\hat{y}) = E(y) - E(\\hat{y}) = X\\beta - X\\beta = 0\\) \\(E(\\hat{\\beta}) = (X^{&#39;}X)^{-1}X^{&#39;}E(y) = (X^{&#39;}X)^{-1}X^{&#39;}X\\beta = \\beta\\) Найдем, к примеру, \\(Cov(\\epsilon, \\hat{\\beta})\\): \\(Cov(\\epsilon,\\hat{\\beta}) = Cov(\\epsilon,(X^{&#39;}X)^{-1}X^{&#39;}y) = Cov(\\epsilon,(X^{&#39;}X)^{-1}X^{&#39;}(X\\beta+\\epsilon)) = Cov(\\epsilon,(X^{&#39;}X)^{-1}X^{&#39;}X\\beta+(X^{&#39;}X)^{-1}X^{&#39;}\\epsilon) = Cov(\\epsilon,\\epsilon)((X^{&#39;}X)^{-1}X^{&#39;})^{&#39;} = \\sigma^{2}I((X^{&#39;}X)^{-1}X^{&#39;})^{&#39;} = \\sigma^{2}X^{&#39;&#39;}((X^{&#39;}X)^{-1})^{&#39;} = \\sigma^{2}X((X^{&#39;}X)^{&#39;})^{-1} = \\sigma^{2}X(X^{&#39;}X)^{-1}\\) Существует 2 традиции матричного дифференцирования, суть различия которых заключается в представлении вектора (матрицы) производной — в виде столбца или в виде строки. Основные различия представлены в следующей таблице: \\[\\begin{align} \\frac{\\partial f}{\\partial x} &amp;= \\begin{pmatrix} \\dfrac{\\partial f}{\\partial x_{1}} \\\\ \\dfrac{\\partial f}{\\partial x_{2}} \\\\ \\vdots \\\\ \\dfrac{\\partial f}{\\partial x_{n}} \\end{pmatrix} \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; \\frac{\\partial f}{\\partial X} &amp;= \\begin{pmatrix} \\dfrac{\\partial f}{\\partial x_{11}} &amp; \\cdots &amp; \\dfrac{\\partial f}{\\partial x_{1k}} \\\\ \\vdots &amp; &amp; \\vdots \\\\ \\dfrac{\\partial f}{\\partial x_{n1}} &amp; \\cdots &amp; \\dfrac{\\partial f}{\\partial x_{nk}} \\end{pmatrix} \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; \\frac{\\partial g}{\\partial x} &amp;= \\begin{pmatrix} \\dfrac{\\partial g_{1}}{\\partial x_{1}} &amp; \\cdots &amp; \\dfrac{\\partial g_{k}}{\\partial x_{1}} \\\\ \\vdots &amp; &amp; \\vdots \\\\ \\dfrac{\\partial g_{1}}{\\partial x_{n}} &amp; \\cdots &amp; \\dfrac{\\partial g_{k}}{\\partial x_{n}} \\end{pmatrix} \\end{align}\\] Если \\(x\\) – вектор, \\(f(x) = Ax\\), то \\(\\dfrac{\\partial f}{\\partial x} = A^{&#39;}\\) Если \\(f(x) = x^{&#39;}Ax\\), то \\(\\dfrac{\\partial f}{\\partial x} = (A + A^{&#39;})x\\) Если \\(f(X) = det(X)\\), то \\(\\dfrac{\\partial f}{\\partial x} =det(X)(X^{-1})^{&#39;}\\) Доказательство второго свойства и не только можно почитать здесь "]
]
