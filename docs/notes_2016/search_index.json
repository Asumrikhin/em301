[
["index.html", "Конспект семинаров по метрике-2016 О проекте", " Конспект семинаров по метрике-2016 Студенты ИП и Борис Демешев 2016-10-05 О проекте Метрика с R :) library(&quot;knitr&quot;) library(&quot;tikzDevice&quot;) activateTikz &lt;- function() { # tikz plots options options(tikzDefaultEngine = &quot;xetex&quot;) # cash font metrics for speed: # options(tikzMetricsDictionary = &quot;./tikz_metrics&quot;) add_xelatex &lt;- c(&quot;\\\\defaultfontfeatures{Ligatures=TeX, Scale=MatchLowercase}&quot;, &quot;\\\\setmainfont{Linux Libertine O}&quot;, &quot;\\\\setmonofont{Linux Libertine O}&quot;, &quot;\\\\setsansfont{Linux Libertine O}&quot;, &quot;\\\\newfontfamily{\\\\cyrillicfonttt}{Linux Libertine O}&quot;, &quot;\\\\newfontfamily{\\\\cyrillicfont}{Linux Libertine O}&quot;, &quot;\\\\newfontfamily{\\\\cyrillicfontsf}{Linux Libertine O}&quot;) options(tikzXelatexPackages = c(getOption(&quot;tikzXelatexPackages&quot;), add_xelatex)) # does remove warnings: # it is important to remove fontenc package wich is loaded by default options(tikzUnicodeMetricPackages = c(&quot;\\\\usetikzlibrary{calc}&quot;, &quot;\\\\usepackage{fontspec, xunicode}&quot;, add_xelatex)) opts_chunk$set(dev = &quot;tikz&quot;, dev.args = list(pointsize = 11)) } colFmt &lt;- function(x, color) { outputFormat &lt;- opts_knit$get(&quot;rmarkdown.pandoc.to&quot;) if (outputFormat == &quot;latex&quot;) { result &lt;- paste0(&quot;\\\\textcolor{&quot;, color, &quot;}{&quot;, x, &quot;}&quot;) } else if (outputFormat %in% c(&quot;html&quot;, &quot;epub&quot;)) { result &lt;- paste0(&quot;&lt;font color=&#39;&quot;, color, &quot;&#39;&gt;&quot;, x, &quot;&lt;/font&gt;&quot;) } else { result &lt;- x } return(result) } outputFormat &lt;- opts_knit$get(&quot;rmarkdown.pandoc.to&quot;) if (outputFormat == &quot;latex&quot;) { activateTikz() # другую тему для ggplot2 выставить? } Данная версия конспекта скомпилирована для html. library(&quot;ggplot2&quot;) # графики library(&quot;sandwich&quot;) # оценка Var для гетероскедастичности library(&quot;lmtest&quot;) # тест Бройша-Пагана library(&quot;dplyr&quot;) # манипуляции с данными library(&quot;broom&quot;) # преобразование всего и вся в стандартные таблички library(&quot;data.table&quot;) # манипуляции с данными library(&quot;reshape2&quot;) # преобразование длинных таблиц в широкие library(&quot;tidyr&quot;) # причесывание данных "],
["ols-intro.html", "1 Метод наименьших квадратов 1.1 Основная задача 1.2 Реализация в R: 1.3 Домашнее задание", " 1 Метод наименьших квадратов Конспект: Бердникович Алеся, Головина Мария дата: 05.09.2016 1.1 Основная задача Маша каждый день ловит покемонов и решает задачи по теории вероятностей. Пусть x и y - случайные величины, \\(x_i\\) - количество решённых в i-тый день задач, а \\(y_i\\) - количество пойманных в i-тый день покемонов. Результаты наблюдения за действиями Маши представлены в таблице: День \\(x_i\\) \\(y_i\\) 1 1 10 2 2 0 3 0 4 Необходимо определить, как количество пойманных за день покемонов зависит от количества решённых за день задач. Предположим, что регрессионная модель имеет линейный вид \\(y_i = \\beta_1+\\beta_2 x_i + \\epsilon_i\\), где коэффициенты \\(\\beta_1, \\beta_2\\) неизвестны и должны быть оценены, а \\(\\epsilon_i\\) - случайная величина. Тогда прогнозируемая зависимость имеет вид \\(\\hat{y}_i=\\hat{\\beta}_1+\\hat{\\beta}_2x_i\\). 1.1.1 Метод наименьших квадратов (OLS): \\(y_i-\\hat{y}_i\\) - ошибка прогноза, которую нужно минимизировать. Штрафная функция: \\[ Q(\\hat{\\beta}_1,\\hat{\\beta}_2) = (y_1-\\hat{y}_1)^2 + (y_2-\\hat{y}_2)^2 + (y_3-\\hat{y}_3)^2 = (y_1-(\\hat{\\beta}_1+\\hat{\\beta}_2 x_1))^2 + (y_2-(\\hat{\\beta}_1+\\hat{\\beta}_2x_2))^2 + (y_3-(\\hat{\\beta}_1+\\hat{\\beta}_2x_3))^2 \\to min_{\\hat{\\beta}_1,\\hat{\\beta}_2} \\] 1.1.2 Метод наименьших модулей (LAD): Альтернативный метод минимизации ошибок прогноза. Отличие заключается в виде штрафной функции: \\[Q(\\hat{\\beta}_1,\\hat{\\beta}_2) = |y_1-\\hat{y_1}|^2 + |y_2-\\hat{y}_2|^2 + |y_3-\\hat{y}_3|^2 \\to min_{\\hat{\\beta}_1,\\hat{\\beta}_2}\\] Найдём \\(\\hat{\\beta}_1,\\hat{\\beta}_2\\) в нашей задаче методом наименьших квадратов: \\[Q(\\hat{\\beta}_1,\\hat{\\beta}_2) = (y_1-(\\hat{\\beta}_1+\\hat{\\beta}_2x_1)^2 + (y_2-(\\hat{\\beta}_1+\\hat{\\beta}_2x_2))^2 + (y_3-(\\hat{\\beta}_1+\\hat{\\beta}_2x_3))^2\\] \\[Q(\\hat{\\beta}_1,\\hat{\\beta}_2) = (10-(\\hat{\\beta}_1+\\hat{\\beta}_2)^2 + (0-(\\hat{\\beta}_1+2\\hat{\\beta}_2))^2 + (4-(\\hat{\\beta}_1))^2 \\to min\\] \\[\\frac{\\partial Q}{\\partial \\hat{\\beta}_1} = -28 + 6\\hat{\\beta}_1 + 6\\hat{\\beta}_2 = 0\\] \\[\\frac{\\partial Q}{\\partial \\hat{\\beta}_2} = -20 + 6\\hat{\\beta}_1 + 10\\hat{\\beta}_2 = 0\\] \\[\\hat{\\beta}_1 = \\frac{20}{3}, \\hat{\\beta}_2 = -2\\] Искомая оценка зависимости числа пойманных покемонов от числа решённых задач: \\[\\hat{y}_i = \\frac{20}{3} - 2x_i\\] 1.2 Реализация в R: x &lt;- c(1, 2, 0) y &lt;- c(10, 0, 4) md &lt;- data.frame(problem = x, pokemon = y) md ## problem pokemon ## 1 1 10 ## 2 2 0 ## 3 0 4 Восстановление линейной зависимости методом наименьших квадратов: model_1_ols &lt;- lm(data = md, pokemon~problem) summary(model_1_ols) ## ## Call: ## lm(formula = pokemon ~ problem, data = md) ## ## Residuals: ## 1 2 3 ## 5.333 -2.667 -2.667 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.667 5.963 1.118 0.465 ## problem -2.000 4.619 -0.433 0.740 ## ## Residual standard error: 6.532 on 1 degrees of freedom ## Multiple R-squared: 0.1579, Adjusted R-squared: -0.6842 ## F-statistic: 0.1875 on 1 and 1 DF, p-value: 0.7399 Подключаем нужный пакет: library(&quot;quantreg&quot;) Если пакета не установлен, то это исправляется командой install.packages(&quot;quantreg&quot;) Восстановление линейной зависимости методом наименьших модулей: model_1_lad &lt;- rq(data = md, pokemon~problem) summary(model_1_lad) ## ## Call: rq(formula = pokemon ~ problem, data = md) ## ## tau: [1] 0.5 ## ## Coefficients: ## coefficients lower bd upper bd ## (Intercept) 4.000000e+00 -1.797693e+308 1.797693e+308 ## problem -2.000000e+00 -1.797693e+308 1.797693e+308 Предположим теперь иную модель зависимости \\(y_i = \\hat{\\beta}x_i\\), ищем оценку единственного неизвестного коэффициента \\(\\hat{\\beta}\\) с помощью метода наименьших модулей. Штрафная функция примет вид \\[Q(\\hat{\\beta}) = |10-\\hat{\\beta}| + |0-\\hat{\\beta}| + |4-0| \\to min\\] Точки изломов функции находятся в нулях подмодульных выражений: \\(\\hat{\\beta}=0\\) и \\(\\hat{\\beta}=10\\). Функция принимает наименьшее значение при \\(\\hat{\\beta}=0\\) (см. график), что говорит об отсутствии зависимости числа пойманных покемонов от числа решённых задач. 1.2.1 График штрафной функции: x &lt;- seq(-10, 20, 0.001) fx &lt;- (x &lt;= 0) * (14 - 3 * x) + (x &gt; 0 &amp; x &lt; 10) * (14 + x) + (x &gt;= 10) * (3*x - 6) plot(x = x, y = fx, xlab = expression(hat(beta)), ylab = &#39;Q&#39;, pch = 20, col = &#39;orchid4&#39;) 1.3 Домашнее задание Вывести общие формулы для коэффициентов \\(\\hat{\\beta}, \\hat{\\beta}_1, \\hat{\\beta}_2\\), используя МНК-оценку, при условии, что: \\(y_i = \\beta x_i + \\epsilon_i, \\ \\hat{y}_i = \\hat{\\beta} x_i\\); \\(y_i = \\hat{\\beta}_1 + \\hat{\\beta}_2 x_i + \\epsilon_i, \\ \\hat{y}_i = \\hat{\\beta}_1+\\hat{\\beta}_2 x_i + \\epsilon_i\\). "],
["ols-geometry.html", "2 Геометрия МНК 2.1 Обозначения 2.2 Ныряем в \\(n\\)-мерное пространство 2.3 Больше проекций", " 2 Геометрия МНК конспект: Света Колесниченко дата: 19 сентября 2016 2.1 Обозначения Варианты представления регрессии: Скалярный вариант: \\(\\hat y_{i} = \\hat \\beta_1 + \\hat \\beta_2\\, x_{i} + \\hat \\beta_3\\, z_{i}\\) Векторный вариант: \\(\\hat y = \\hat \\beta_1\\, e + \\hat \\beta_2\\, x + \\hat \\beta_3\\, z\\) \\[ \\begin{matrix} &amp; e = \\vec 1 = \\\\ \\end{matrix} \\begin{pmatrix} 1 \\\\ \\vdots \\\\ 1 \\end{pmatrix} \\begin{matrix} \\textit{ - единичный вектор размерности $n \\times 1$}, \\end{matrix} \\] \\[ \\begin{matrix} &amp; x = \\\\ \\end{matrix} \\begin{pmatrix} x_{1} \\\\ \\vdots \\\\ x_{n} \\end{pmatrix} \\begin{matrix} &amp; y = \\\\ \\end{matrix} \\begin{pmatrix} y_{1} \\\\ \\vdots \\\\ y_{n} \\end{pmatrix} \\begin{matrix} &amp; z = \\\\ \\end{matrix} \\begin{pmatrix} z_{1} \\\\ \\vdots \\\\ z_{n} \\end{pmatrix} \\textit{ - векторы переменных} \\] Количество наблюдений = \\(n\\), количество коэффициентов \\(\\beta\\) = количество регрессоров = \\(k\\). Матричный вариант: \\(\\hat y = X\\, \\hat \\beta\\) \\[ \\begin{matrix} &amp; \\hat \\beta = \\\\ \\end{matrix} \\begin{pmatrix} \\hat \\beta_{1} \\\\ \\vdots \\\\ \\hat \\beta_{k} \\end{pmatrix} \\begin{matrix} \\textit{ - вектор размера } k\\times 1, \\end{matrix} \\] \\[ \\begin{matrix} &amp; X = \\\\ \\end{matrix} \\begin{pmatrix} 1 &amp; x_{1} &amp; z_{1} \\\\ \\vdots &amp; \\vdots &amp; \\vdots \\\\ 1 &amp; x_n &amp; z_{n} \\end{pmatrix} \\begin{matrix} \\textit{ - матрица размера } n \\times k \\end{matrix} \\] Конвенция об обозначениях: \\(y, \\beta, \\hat \\beta, x, z\\) - векторы \\(y_{i}, \\beta_{j}, \\hat \\beta_{7}, x_{45}, z_{37}\\) - числа (скаляры) \\(\\Omega, X, H\\) - матрицы 2.2 Ныряем в \\(n\\)-мерное пространство \\[ \\min_{i\\in I} \\sum_{i=1}^n (y_{i} - \\hat y_{i})^{2} = \\min_{i\\in I} \\sum_{i=1}^n |y_{i} - \\hat y_{i}|^{2} \\textit{ - минимизируем квадрат длины вектора} \\] \\[ \\begin{pmatrix} \\bar y \\\\ \\vdots \\\\ \\bar y \\end{pmatrix} \\begin{matrix} &amp; = \\bar y \\cdot \\\\ \\end{matrix} \\begin{pmatrix} 1 \\\\ \\vdots \\\\ 1 \\end{pmatrix} \\begin{matrix} &amp; = \\bar y \\cdot \\vec 1 \\end{matrix} \\] \\(\\bar y = \\hat y\\) - т.к. \\(\\bar y \\cdot \\vec 1 = \\hat y \\cdot \\vec 1\\) среднее значение = среднее значение прогнозов (\\(\\hat y_{i}\\)) Картиночка Лапы “Лапа” = \\(Lin (e, x, z) \\leftarrow\\) выбираем через e, x, z положение \\(\\hat y\\) \\(\\hat y\\) - проекция y на “лапу” y - линейная комбинация e, x, z \\(\\rightarrow\\) лежит в линейной оболочке этих векторов \\(\\hat \\varepsilon = y - \\hat y\\) - вектор “остатков”/ошибок прогнозов/resideals \\(\\hat \\varepsilon \\,\\bot\\, e, \\hat \\varepsilon \\,\\bot\\, x, \\hat \\varepsilon \\,\\bot\\, y\\) \\(\\hat \\varepsilon \\cdot \\vec 1 = 0\\), \\(\\hat \\varepsilon \\cdot x = 0\\), \\(\\hat \\varepsilon \\cdot z = 0\\) \\(\\leftarrow\\) скалярное произведение векторов (ссыль подробнее) перпендикулярных векторов равно 0. \\(\\hat \\varepsilon \\, \\bot \\, \\textit{Лапа} \\rightarrow \\hat \\varepsilon \\, \\bot \\, \\textit{любому вектору, лежащему в Лапе}\\) Великая Теорема о 3 перпендикулярах и аж в 2 формулировках и с чертёжиком. \\[ \\sum_{i=1}^n \\hat \\varepsilon_{i}=0 , \\sum_{i=1}^n \\hat \\varepsilon_{i} x_{i}=0 \\] \\[ \\begin{matrix} &amp; X&#39; = \\\\ \\end{matrix} \\begin{pmatrix} 1 &amp; \\dots &amp; 1\\\\ x &amp; \\dots &amp; x_{n} \\\\ z &amp; \\dots &amp; z_{n} \\end{pmatrix} \\begin{matrix} \\textit{ - единичный вектор размерности $n \\times 1$} \\end{matrix} \\begin{matrix} &amp; \\hat \\varepsilon = \\\\ \\end{matrix} \\begin{pmatrix} \\hat \\varepsilon_{1} \\\\ \\vdots \\\\ \\hat \\varepsilon_{n} \\end{pmatrix} \\begin{matrix} \\textit{ - единичный вектор размерности $n \\times 1$} \\end{matrix} \\] Условие ортогональности: \\(X&#39; \\cdot \\hat \\varepsilon = 0\\) - размерность этого нуля - \\(k \\times 1\\) \\(\\hat y = X \\cdot \\hat \\beta \\rightarrow \\hat \\beta = \\frac{\\hat y}{X} = \\frac{\\hat \\varepsilon - Y}{X}\\) 2.2.1 Упражнение 1 Выведите \\(\\hat \\beta\\) из \\(X&#39;\\cdot(y-X\\cdot\\hat \\beta) = 0\\) \\(\\hat \\varepsilon = y - \\hat y = y - X\\cdot\\hat \\beta\\) \\(X\\) - задает “лапу”. \\(\\hat \\beta\\) - отвечает за то, с каким весом в \\(\\hat y\\) входят базисные векторы «лапы». \\(\\hat \\beta = (\\sum_{i=1}^n x^2_{i})^{-1}\\, \\sum_{i=1}^n x_{i} y_{i}=0\\) - для \\(\\hat y_{i} = \\hat \\beta \\, x_{i}\\) \\(X&#39;\\cdot y = X&#39;\\cdot X\\cdot \\hat \\beta\\) \\((X&#39; \\cdot X)^{-1}\\cdot X&#39; \\cdot y = (X&#39; \\cdot X)^{-1}\\cdot X&#39;\\cdot X \\cdot \\hat \\beta\\) \\(\\hat \\beta = (X&#39; \\cdot X)^{-1}\\cdot X&#39;\\cdot y\\) 2.3 Больше проекций 2.3.1 Упражнение 2 Спроецируйте вектор \\(\\,\\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\end{pmatrix}\\,\\) на прямую, порождённую вектором \\(\\,\\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{pmatrix}\\,.\\) Визуализация задачи \\[ \\begin{matrix} &amp; \\hat \\beta = \\\\ \\end{matrix} \\begin{pmatrix} 1 &amp; 1 &amp; 1 &amp; 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{pmatrix} \\begin{pmatrix} 1 &amp; 1 &amp; 1 &amp; 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\end{pmatrix} \\begin{matrix} = 4^{-1} \\cdot 10 \\cdot \\frac{1}{4} \\cdot 10 = 2.5 \\\\ \\end{matrix} \\] \\[ \\begin{matrix} &amp; \\hat y = X \\cdot \\hat \\beta = \\\\ \\end{matrix} \\begin{pmatrix} 2.5 \\\\ 2.5 \\\\ 2.5 \\\\ 2.5 \\end{pmatrix} \\begin{matrix} = \\\\ \\end{matrix} \\begin{pmatrix} \\bar y \\\\ \\bar y \\\\ \\bar y \\\\ \\bar y \\end{pmatrix} \\] \\[ \\bar y = \\frac{1+2+3+4}{4} = 2.5 \\] Проекция вектора на прямую из единиц даёт вектор из средних. By the way, крутые читщиты по матрицам и основам линейной алгебры. 2.3.2 Упражнение 3 Сформулируйте все теоремы Пифагора \\(\\{\\hat \\varepsilon, \\hat y - \\bar y \\cdot \\vec 1, y - \\bar y \\cdot \\vec 1\\}\\,\\) Чертёжик одного из треугольников По Теореме Пифагора: \\[ |y - \\bar y \\cdot \\vec 1|^{2} = |\\hat \\varepsilon|^2 + |\\hat y - \\bar y \\cdot \\vec 1|^2 \\] \\[ \\sum_{i=1}^n (y_{i}- \\bar y)^2 = \\sum_{i=1}^n \\hat \\varepsilon^2_{i} + \\sum_{i=1}^n (\\hat y_{i}- \\bar y)^2 \\] \\[ \\begin{matrix} &amp; y_{i}- \\bar y = \\\\ \\end{matrix} \\begin{pmatrix} y_{1} - \\bar y\\\\ \\vdots \\\\ y_{n} - \\bar y \\end{pmatrix} \\] Полное задание см. в Задачнике по координатам: 4.23, 4.24, 4.25 Коэффициент детерминации (\\(R^2\\)) - примитивный показатель качества прогнозов. \\[ R^2 = \\frac{ESS}{TSS} = \\frac{\\sum_{i=1}^n \\hat \\varepsilon^2_{i}}{\\sum_{i=1}^n (y_{i}- \\bar y)^2} = \\frac{\\sum_{i=1}^n (y_{i}- \\hat y)^2}{\\sum_{i=1}^n (y_{i}- \\bar y)^2} = \\frac{\\text{residial sum of squares (cумма квадратов остатков)}}{\\text{total sum of squares (полная сумма квадратов)}} \\] \\[ ESS = \\sum_{i=1}^n (\\hat y_{i}- \\bar y)^2 = \\, \\text{explained sum of squares (&quot;объясненная&quot; сумма квадратов)} \\] В МНК работает (и точно только в нём!) соотношение: \\(RSS + ESS = TSS\\). В МНК решается задача минимизации RSS. Если прогнозы \\(\\hat y_i\\) идеально совпадают с \\(y_i\\), то \\(R^2 = 1 \\Rightarrow ESS = TSS\\). \\(R^2 \\in [0;1]\\, , R^2 = \\cos^2 \\rho\\) \\(\\hat y\\) ближе к \\(y\\) с ростом «лапы» \\(\\Rightarrow\\) \\(\\angle \\rho \\downarrow \\, \\rightarrow R^2\\,\\) т.к. \\(\\cos^2 \\rho \\uparrow\\) 2.3.2.1 ДЗ: 1.1, 1.2, 1.7, 1.12, 1.13, 4.13 (1-6), 4.23, 4.24, 4.25 из Задачника 2.3.2.1.1 Полезные ссылки: Репозиторий курса метрики и теории вероятностей "]
]
