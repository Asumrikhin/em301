# Оценивание ARMA моделей в R

Загружаем библиотеку `quantmod`, которая позволяет быстро скачивать данные по цене акций, курсам валют и строить всякие графики для технического анализа
```{r, message=FALSE}
library(quantmod)
library(knitr)
opts_chunk$set(cache=TRUE, dev="png", warning=FALSE)
# cache=TRUE --- включает кэширование для быстрого повторного запуска  программы
# dev="png" переводит все графики в растровый формат png, там почти исчезают проблемы с кириллицей
# warning=FALSE убирает вывод на экран предупреждений

```

Загружаем данные по акциям Гугла:
```{r}
getSymbols(Symbols="GOOG", from="2012-01-01", to="2013-03-01")
```

Смотрим, что там загрузилось:
```{r}
head(GOOG)
```

Для удобства называем скорректированную на дивидент цену одной буквой
```{r}
y <- GOOG$GOOG.Adjusted
```

Строим график
```{r}
plot(y)
```

Даже по графику видно, что ряд y не стационарный. Например, если мысленно поделить график на две части, то среднее в левой явно не равно среднему в правой. 

Такой график характерен для случайного блуждания, т.е. процесса примерно вида $y_t=y_{t-1}+u_t$.

Для нестационарного ряда не имеет смысла расчёт ACF и PACF, так как корреляция между случайными величинами отстоящими на $k$ шагов может меняться во времени. Тем не менее, глянем на графики:

Пара бессмысленных и беспощадных графиков
```{r}
acf(y)
pacf(y)
```
Несмотря на свою бессмысленность, они полезны тем, что по ним можно опознать случайное блуждание. Если мы тупо попросили компьютер построить графики ACF и PACF и оказалось, что ACF крайне медленно убывает, а PACF обрывается в ноль после первой частной корреляции, то, вероятно, перед нами случайное блуждание.


Переходим к разностям, $z_t=y_t - y_{t-1}$
```{r}
z=diff(y)
```
 
График для $z$ уже похоже на стационарный, $z$ крутитися вокруг среднего в полосе примерно постоянной ширины.
```{r}
plot(z)
```
 

Графики ACF и PACF для $z$
```{r}
acf(z,na.action=na.omit)
pacf(z,na.action=na.omit)
```

Оцениваем модель arima(1,1,0), arima(0,1,1), arima(1,1,1)
```{r}
model1 <- arima(y,order=c(1,1,0))
model2 <- arima(y,order=c(0,1,1))
model3 <- arima(y,order=c(1,1,1))
```

Смотрим на оценки ARIMA(1,1,1)
```{r}
print(model3)
```
Видим, что коэффициенты значимы. К сожалению, они не очень хорошо интерпретируются.


Строим прогноз по модели ARIMA(1,1,1) на 5 шагов вперед
```{r}
predict(model3,n.ahead=5)
```


### Лирическое отступление. Чудо, при котором туземцы воздевают руки к небу.

Компьютер оценивает все модели минимизируя или максимизируя какую-нибудь функцию. При применении МНК минимизируется сумма квадратов остатков, в методе максимального правдподобия --- максимизируется функция правдоподобия. Для оптимизации придумано много алгоритмов. Но бывает, что они не срабатывают на какой-нибудь простой функции...

Найдите минимум данной функции устно, не используя компьютер:
$$
f(x_1,x_2,x_3)=0.01\cdot (x_1-0.5)^2+|x_1^2-x_2|+|x_1^2-x_3|
$$

А теперь попробуем с компьютером:
```{r}
f <- function(x) {
  return(0.01*(x[1]-0.5)^2+
           abs(x[1]^2-x[2])+
           abs(x[1]^2-x[3]))
}
nlm(f,c(0,0,0))
```

Это не проблема R, это проблема алгоритмов поиска экстремума --- им тяжело работать с недифференциируемой функцией, у которой скачком меняется скорость роста.



