# Преобразования таблиц

Загружаем нужные библиотеки:
```{r, message=FALSE}
library(knitr)
opts_chunk$set(cache=FALSE) # кэшируем куски Rmd файла для скоростной компиляции после

library(plyr) # ddply
library(reshape) # melt - cast
library(kernlab) # SVM
library(ggplot2) # графики
```


## Разделяй и властвуй! Затем соединяй!

Многие преобразования могут быть описаны с помощью принципа 
"Разделяй и властвуй! Затем соединяй!":
  * Разделяем большую табличку на много маленьких
  * На каждой маленькой табличке делаем простые вычисления
  * Соединяем результат в одну большую табличку

Загрузим данные по стоимости квартир в Москве:
```{r}
file <- "~/science/econometrix/em301/datasets/flats_moscow.txt"
h <- read.table(file,header=TRUE)
```


Пример. Подсчёт описательных статистик внутри групп

Мы разбиваем большую таблицу на маленькие таблички по переменным `code` (географический район квартиры) и `brick` (кирпичность). По каждой маленькой табличке считаем число наблюдений и среднюю цену квартиры. Затем сводим в итоговую таблицу:
```{r}
h.summ <-ddply(h,~code+brick,summarize,
      n=length(price),mean=mean(price))
head(h.summ)
```

Пример. Корректировка наблюдения на групповое среднее. Из цены каждой квартиры будет вычтена средняя цена по квартирам данного района и данной кирпичности.
```{r}
h.new <- ddply(h,~code+brick,transform,
            delta=price-mean(price))
head(h.new)
```

Итого:
* опция `transform` в финальной табличке создаёт столько строк, сколько было в изначальной табличке
* опция `summarize` в финальной табличке создаёт столько строк, на сколько мелких табличек разрезали изначальную 

Из опыта. Для подсчета числа наблюдений бывает полезна функция `nrow()`. Функция `nrow()` работает только без имени переменной и без `summarize`.


## Длинная и широкая

### Теорема. Крокодил более широкий чем длинный.

Доказательство. 

Шаг 1. Крокодил более широкий, чем зеленый. Зеленый он только снаружи, а широкий он и снаружи и внутри.

Шаг 2. Крокодил более зеленый, чем длинный. Зеленый он и в длину, и в ширину, а длинный - только в длину.

Шаг 3. Отношение "больше" транзитивно :)


### Таблицы данных бывают "широкие" и "длинные".

Пример "широкой" таблицы

```{r}
tab.wide <- smiths
head(tab.wide)
```


Пример "длинной" таблицы

```{r}
names(airquality) <- tolower(names(airquality))
tab.long <- melt(airquality, id=c("month", "day"), na.rm=TRUE)

head(tab.long)
```


Данные удобнее обрабатывать в "длинном" формате. В "длинном" формате при получении новых наблюдений не нужно добавлять столбцы в data.frame. "Широкий" формат изредка бывает удобен для представления результатов. Или клиент так хочет.

Чтобы табличка растаяла из "широкого" формата и стекла вниз в "длинный" её можно растопить командой `melt` из пакета `reshape`.


```{r}
tab.long2 <- melt(tab.wide, id="subject")
head(tab.long2)
```

Обратное действие выполняется командой `cast`.


```{r}
tab.wide2 <- cast(tab.long, day + month ~ variable, mean)
head(tab.wide2)
```


# Выбор между точностью подгонки и простотой модели

Во многих моделях есть параметр, отвечающий за простоту модели.  Чем проще модель, тем хуже модель описывает выборку, по которой она оценивалась.

В линейной регрессии параметр сложности модели --- это количество регрессоров, $k$. Чем больше регрессоров, тем ниже сумма квадратов остатков, $RSS$.

На гистограмме параметр сложности --- число столбцов.

В ridge regression и LASSO параметр простоты --- это $\lambda$. Действительно, LASSO минимизирует
\[
\sum_{i=1}^n (y_i-\hat{y}_i)^2+\lambda \sum_{j=1}^k |\hat{\beta}_j|
\]
Значит, чем больше параметр $\lambda$, тем больше стремление алгоритма LASSO занулить некоторые $\hat{\beta}_j$.

Этот параметр сложности не так легко оценить, как коэффициенты модели. Если наивно попытаться выбрать параметр сложности так, чтобы модель как можно лучше описывала бы выборку, по которой она оценивалась... То ничего хорошего не выйдет. Окажется, что оптимальное количество регрессоров равно плюс бесконечности, а оптимальное $\lambda$ в LASSO равно нулю. В регрессии одно из решений этой проблемы --- это проверка гипотез о значимости коэффициентов.

Есть и универсальный способ выбора сложности модели --- кросс валидация (перекрёстная проверка, cross validation). Её идея состоит в том, что надо оценивать качество прогнозов не по той же выборке, на которой оценивалась модель, а на новых наблюдениях.

# k-кратная кросс-валидация

* Разбиваем случайным образом всю выборку на k частей. Сразу мораль: используйте `set.seed()` для воспроизводимости эксперимента.
* Прогнозы для первой части выборки строим, оценивая модель по наблюдениям всех остальных частей. Прогнозы для второй части выборки строим, оценивая модель по наблюдениям всех частей кроме второй. И так далее. 
* Получаем по одному прогнозу для каждого наблюдения. 
* Считаем сумму квадратов ошибок прогнозов или другой показатель их качества.

Популярные значения k:
* 10-кратная кроссвалидация
* k равно числу наблюдений. Т.е. модель оценивается по всем наблюдениям кроме одного. Для этого невключенного наблюдения считается ошибка прогноза. Исключая по очереди то одно, то другое наблюдение, получаем ошибку прогноза для каждого наблюдения. По этим ошибкам считаем сумму квадратов. При этом подходе алгоритм не является случайным и зачастую есть готовые формулы для суммы квадратов ошибок прогнозов.


## Кросс-валидация на примере SVM



Создаём табличку перебираемых $C$ и $\sigma$:
```{r}
C <- c(1,10,100)
sigma <- c(0.1,1,10)
d <- expand.grid(C,sigma) # Случайно не декартово ли произведение это? :)
colnames(d) <- c("C","sigma")
head(d)
```

Пробуем 10-кратную кросс-валидацию для конкретных $C$ и $\sigma$:
```{r}
set.seed(33222233) # любимый seed Фрекен Бок
m1 <- ksvm(data=h,brick~.,
           kernel=rbfdot,
           kpar=list(sigma=1),
           C=1,cross=10)
cross(m1) # cross validation error
```


Для удобства оформляем это в функцию двух переменных
```{r}
f_cross <- function(sig,C) {
  model <- ksvm(data=h,brick~.,
             kernel=rbfdot,
             kpar=list(sigma=sig),
             C=C,cross=10)
  return(cross(model))

}
f_cross(1,1) # тестируем сделанную функцию
```

Применяем её ко всем возможным $C$ и $\sigma$
```{r}
d <- ddply(d,~C+sigma,transform,cr=f_cross(sigma,C))
head(d)
```


Ссылки на дополнительные материалы:
* Hadley Wickham, [The Split-Apply-Combine Strategy for Data Analysis](http://www.jstatsoft.org/v40/i01/paper) Стратегия "Разделяй и властвуй! Затем соединяй!"
* Hadley Wickham, [Reshaping Data with the reshape Package](http://www.jstatsoft.org/v21/i12/paper) Растопи и cast табличку с данными.

ps Придумайте хороший перевод названия функции `cast` на русский язык.
